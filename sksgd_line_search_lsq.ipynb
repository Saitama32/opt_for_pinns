{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d1c57d-5d8c-4fd1-8154-7643d38e290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.func import vmap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43dcd14a-90ed-4663-84dc-6b0299913436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60883610-046b-45ff-8373-e40fa26a2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchySGD(Optimizer):\n",
    "    \"\"\"Implements SketchySGD. We assume that there is only one parameter group to optimize.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rank (int): sketch rank\n",
    "        rho (float): regularization\n",
    "        lr (float): learning rate\n",
    "        weight_decay (float): weight decay parameter\n",
    "        hes_update_freq (int): how frequently we should update the Hessian approximation\n",
    "        momentum (float): momentum parameter\n",
    "        proportional (bool): option to maintain lr to rho ratio, even when lr decays\n",
    "        chunk_size (int): number of Hessian-vector products to compute in parallel\n",
    "                          if set to None, binary search will be used to find the maximally allowed value\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1, lr = 0.01, weight_decay = 0.0,\n",
    "                 hes_update_freq = 100, momentum = 0.0, proportional = False, \n",
    "                 chunk_size = None, line_search_fn = None, verbose = False):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho, lr = lr, weight_decay = weight_decay, \n",
    "                        hes_update_freq = hes_update_freq, proportional = proportional,\n",
    "                        chunk_size = chunk_size, momentum = momentum, line_search_fn = line_search_fn)\n",
    "        self.rank = rank\n",
    "        self.hes_update_freq = hes_update_freq\n",
    "        self.proportional = proportional\n",
    "        self.chunk_size = chunk_size\n",
    "        self.ratio = rho / lr\n",
    "        self.hes_iter = 0\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        self.counter = 0\n",
    "        self.momentum = momentum\n",
    "        self.momentum_buffer = None\n",
    "        self.line_search_fn = line_search_fn\n",
    "        self.large_step_size_index = []\n",
    "        self.verbose = verbose\n",
    "        super(SketchySGD, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        grad_tuple = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss, grad_tuple = closure()\n",
    "\n",
    "        # update Hessian approximation, if needed\n",
    "        g = torch.cat([gradient.view(-1) for gradient in grad_tuple if gradient is not None])\n",
    "        if self.hes_iter % self.hes_update_freq == 0:\n",
    "            params = []\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "\n",
    "            # update preconditioner\n",
    "            self._update_preconditioner(params, g)\n",
    "\n",
    "        g = g.detach()\n",
    "\n",
    "        # update momentum buffer\n",
    "        if self.momentum_buffer is None: \n",
    "            self.momentum_buffer = g\n",
    "        else:\n",
    "            self.momentum_buffer = self.momentum * self.momentum_buffer + g\n",
    "\n",
    "        # one step update\n",
    "        for group_idx, group in enumerate(self.param_groups):\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            # Adjust rho to be proportional to lr, if necessary\n",
    "            if self.proportional:\n",
    "                rho = lr * self.ratio\n",
    "            else:\n",
    "                rho = group['rho']\n",
    "\n",
    "            # compute gradient as a long vector\n",
    "            # g = torch.cat([p.grad.view(-1) for p in group['params'] if p.grad is not None]) # only get gradients if they exist!\n",
    "            # calculate the search direction by Nystrom sketch and solve\n",
    "            UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "            g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "            \n",
    "            # use backtracking line search to find an appropriate step-size\n",
    "            step_size = lr\n",
    "            direction = g_new.detach().neg()\n",
    "            if self.line_search_fn is not None: \n",
    "                # possibly reduce the initial step size for the very first iteration\n",
    "                if self.hes_iter == 0: \n",
    "                    step_size = min(1., 1. / g.abs().sum()) * lr\n",
    "                # get a copy of current param values (as evaluating loss requires overwriting current param)\n",
    "                current_params = self._clone_param(group_idx)\n",
    "                # compute the dot product of gradient\n",
    "                grad_dir_prod = g.dot(direction).item()\n",
    "                # define the objective/loss evaluation function\n",
    "                def obj_func(current_params, step_size, direction): \n",
    "                    # set new param values\n",
    "                    self._add_grad(group_idx, step_size, direction)\n",
    "                    # obtain objective/loss\n",
    "                    obj_value, grad = closure()\n",
    "                    flat_grad = torch.cat([gradient.view(-1) for gradient in grad if gradient is not None]).detach()\n",
    "                    # revert back to the original/current param values\n",
    "                    self._set_param(group_idx, current_params)\n",
    "\n",
    "                    # return obj_value\n",
    "                    return float(obj_value), flat_grad\n",
    "                # search for the sufficient decrease step-size\n",
    "                if self.line_search_fn == \"backtracking\": \n",
    "                    step_size = self._backtracking(obj_func, current_params, step_size, direction, loss.item(), grad_dir_prod, use_interpolation=True)\n",
    "                elif self.line_search_fn == \"strong_wolfe\": \n",
    "                    step_size = self._strong_wolfe(obj_func, current_params, step_size, direction, loss.item(), g, grad_dir_prod)\n",
    "                else: \n",
    "                    raise Exception(f'Line search function \\\"{self.line_search_fn}\\\" is not supported.')\n",
    "            # store step-size in state dict\n",
    "            self.state[group_idx]['step_size'] = step_size\n",
    "            # update model parameters with either fixed or found step-size\n",
    "            self._add_grad(group_idx, step_size, direction)\n",
    "        \n",
    "        self.hes_iter += 1\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def _update_preconditioner(self, params, gradsH):\n",
    "        p = gradsH.shape[0]\n",
    "        # Generate test matrix (NOTE: This is transposed test matrix)\n",
    "        Phi = (torch.randn(self.rank, p) / (p ** 0.5)).to(params[0].device)\n",
    "        \n",
    "        if self.chunk_size is None: \n",
    "            self._set_chunk_size(params, gradsH, Phi)\n",
    "\n",
    "        # Calculate sketch (NOTE: This is transposed sketch)\n",
    "        Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "\n",
    "        # Calculate shift\n",
    "        shift = torch.finfo(Y.dtype).eps * 10\n",
    "        Y_shifted = Y + shift * Phi\n",
    "        # Calculate Phi^T * H * Phi (w/ shift) for Cholesky\n",
    "        choleskytarget = torch.mm(Y_shifted, Phi.t())\n",
    "        # Perform Cholesky, if fails, do eigendecomposition\n",
    "        # The new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # print(eigs)\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        try: \n",
    "            B = torch.linalg.solve_triangular(C, Y_shifted, upper = False, left = True)\n",
    "        # temporary fix for issue @ https://github.com/pytorch/pytorch/issues/97211\n",
    "        except: \n",
    "            B = torch.linalg.solve_triangular(C.to('cpu'), Y_shifted.to('cpu'), upper = False, left = True).to(C.device)\n",
    "        _, S, UT = torch.linalg.svd(B, full_matrices = False) # B = V * S * U^T b/c we have been using transposed sketch\n",
    "        self.U = UT.t()\n",
    "        self.S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "        \n",
    "        if self.verbose: \n",
    "            # print low-rank Hessian approximation (without rho)\n",
    "            print(f'Hessian Approximation: {torch.mm(torch.mm(self.U, torch.diag(self.S)), self.U.t())}')\n",
    "\n",
    "    def _hvp_vmap(self, grad_params, params):\n",
    "        return vmap(lambda v: hvp(grad_params, params, v), in_dims = 0, chunk_size=self.chunk_size)\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function for finding the maximally allowed chunck_size. \n",
    "\n",
    "    INPUT: \n",
    "    - params: ...\n",
    "    - gradsH: ...\n",
    "    - Phi: ...\n",
    "    - safety_margin: float; free / total GPU memory ratio -- if the free memory is lower than the margin, \n",
    "                     then this suggests actual chunk size should be set at a multiplicative factor of the found value\n",
    "    - safety_margin_factor: integer; multiplicative factor to use when the free memory is low\n",
    "    \"\"\"\n",
    "    def _set_chunk_size(self, params, gradsH, Phi, safety_margin=0.05, safety_margin_factor=0.95): \n",
    "        # start with the rank\n",
    "        self.chunk_size = self.rank\n",
    "        # set bounds for the search\n",
    "        max_size = self.rank\n",
    "        min_size = 1\n",
    "        while(True): \n",
    "            # update lower bound if attempted computation was successful\n",
    "            try: \n",
    "                self._hvp_vmap(gradsH, params)(Phi)\n",
    "                min_size = self.chunk_size\n",
    "                # search range has converged to a single point\n",
    "                if max_size - min_size <= 1: \n",
    "                    # grab memory information\n",
    "                    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "                    if free_mem / total_mem < safety_margin: \n",
    "                        min_size = int(safety_margin_factor * min_size)\n",
    "                    # create some safety margin (e.g. 95% of the found size)\n",
    "                    self.chunk_size = max(1, min_size)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    break\n",
    "            # update upper bound if attempted computation ran out of memory\n",
    "            except RuntimeError as e:\n",
    "                if str(e).startswith('CUDA out of memory.') and self.chunk_size > 1:\n",
    "                    max_size = self.chunk_size\n",
    "                    torch.cuda.empty_cache()\n",
    "                # terminate if other runtime error occurred or chunk_size = 1 still ran out of memory\n",
    "                else: \n",
    "                    raise e\n",
    "            # halve the search range\n",
    "            self.chunk_size = int(0.5 * (min_size + max_size))\n",
    "        # report final chunk size\n",
    "        print(f'SketchySGD: chunk size has been set to {self.chunk_size}.')\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function for performing the backtracking line search (Armijo rule).\n",
    "\n",
    "    INPUT: \n",
    "    - current_params: Tensor; long flattened vector of the model parameters\n",
    "    - step_size: Tensor (of size 1); initial step-size (learning rate) for the step\n",
    "    - direction: Tensor; long flattened vector of the update direction\n",
    "    - obj_value: Tensor (of size 1); objective value (loss) at the current value of the parameters\n",
    "    - grad_dir_prod: Tensor (of size 1); result of the dot product of the gradient of the objective function and direction\n",
    "    - c1: float; constant used in the evaluation of the sufficient decrease condition\n",
    "    - alpha: float; constant multiplicative factor used to decrease step-size after each unsuccessful search\n",
    "    - max_ls: integer; maximum number of line searches\n",
    "    OUTPUT: \n",
    "    - step_size: Tensor; resulting step-size from the search\n",
    "    \"\"\"\n",
    "    def _backtracking(self, obj_func, current_params, step_size, direction, obj_value, grad_dir_prod, c1=1e-4, alpha=0.5, max_ls=30, use_interpolation=True): \n",
    "        # compute objective function with the initial step-size\n",
    "        obj_value_new = obj_func(current_params, step_size, direction)[0]\n",
    "        # start line search\n",
    "        ls_iter = 0\n",
    "        while ls_iter < max_ls:\n",
    "            # evaluate the sufficient decrease condition\n",
    "            if obj_value_new > (obj_value + c1 * step_size * grad_dir_prod): \n",
    "                # find step size using quadratic interpolation\n",
    "                if use_interpolation: \n",
    "                    step_size = self._quadratic_interpolate(obj_value, obj_value_new, grad_dir_prod, step_size)\n",
    "                # decrease step-size by a constant multiplicative factor alpha\n",
    "                else: \n",
    "                    step_size = alpha * step_size        \n",
    "                # compute objective function with the new step-size\n",
    "                obj_value_new = obj_func(current_params, step_size, direction)[0]\n",
    "                ls_iter += 1\n",
    "            # otherwise the condition is satisfied\n",
    "            else: \n",
    "                break\n",
    "\n",
    "        return step_size\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function for making a copy of the params in the given param group. \n",
    "\n",
    "    INPUT: \n",
    "    - group_idx: integer; index of the param group\n",
    "    OUTPUT: \n",
    "    - step_size: list\n",
    "    \"\"\"\n",
    "    def _clone_param(self, group_idx):\n",
    "        return [p.clone(memory_format=torch.contiguous_format) for p in self.param_groups[group_idx]['params']]\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function for updating param values in the given param group.\n",
    "    Specifically, x <- x + step_size * (update + weight_decay + x)\n",
    "    Here we use the weight decay, and it is not the same as L2 regularization.\n",
    "\n",
    "    INPUT: \n",
    "    - group_idx: integer; index of the param group\n",
    "    - step_size: float; step-size\n",
    "    - update: list; long Tensor representing the update direction\n",
    "    \"\"\"\n",
    "    def _add_grad(self, group_idx, step_size, update):\n",
    "        weight_decay = self.param_groups[group_idx]['weight_decay']\n",
    "        offset = 0\n",
    "        for p in self.param_groups[group_idx]['params']:\n",
    "            numel = p.numel()\n",
    "            p.data.add_(update[offset:offset + numel].view_as(p) + weight_decay * p.data, alpha=step_size)\n",
    "            offset += numel\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function for assigning value to params in the given param group.\n",
    "    Specifically, x <- params_data.\n",
    "\n",
    "    INPUT: \n",
    "    - group_idx: integer; index of the param group\n",
    "    - params_data: list; long Tensor representing the value to assign\n",
    "    \"\"\"\n",
    "    def _set_param(self, group_idx, params_data):\n",
    "        for p, pdata in zip(self.param_groups[group_idx]['params'], params_data):\n",
    "            p.data.copy_(pdata)\n",
    "    \n",
    "        \"\"\"\n",
    "    Helper function for performing quadratic interpolation. \n",
    "    Specifically, compute x_hat_min = (-g1 * (x ** 2)) / (2 * (f2 - f1 - g1 * x)) > 0\n",
    "    If x_hat_min < x_lower_bound, then x_new = x_lower_bound\n",
    "    If x_lower_bound <= x_hat_min <= x_upper_bound, then x_new = x_hat_min\n",
    "    If x_hat_min > x_upper_bound, then x_new = x_upper_bound\n",
    "\n",
    "    INPUT: \n",
    "    - f1: float; function value at initial x, f(x_0)\n",
    "    - f2: float; function value at current x, f(x)\n",
    "    - g1: float; gradient at initial x, grad(f(x_0))\n",
    "    - x: float; current x\n",
    "    - bounds: tuple of floats; lower and upper bounds of the new x\n",
    "    OUTPUT: \n",
    "    - x_new: float; value of the x in the bound that minimizes the interpolation\n",
    "    \"\"\"\n",
    "    def _quadratic_interpolate(self, f1, f2, g1, x, bounds=None): \n",
    "        if bounds is not None:\n",
    "            x_lower_bound, x_upper_bound = bounds\n",
    "        else: \n",
    "            x_lower_bound = 0.5 * x\n",
    "            x_upper_bound = 0.95 * x\n",
    "\n",
    "        x_hat_min = (-g1 * (x ** 2)) / (2 * (f2 - f1 - g1 * x))\n",
    "\n",
    "        return min(max(x_hat_min, x_lower_bound), x_upper_bound)\n",
    "    \n",
    "    def _cubic_interpolate(self, f1, f2, g1, g2, x1, x2, bounds=None): \n",
    "        if bounds is not None:\n",
    "            xmin_bound, xmax_bound = bounds\n",
    "        else:\n",
    "            xmin_bound, xmax_bound = (x1, x2) if x1 <= x2 else (x2, x1)\n",
    "\n",
    "        d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)\n",
    "        d2_square = d1**2 - g1 * g2\n",
    "            \n",
    "        if d2_square >= 0:\n",
    "            d2 = d2_square.sqrt()\n",
    "            if x1 <= x2:\n",
    "                min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))\n",
    "            else:\n",
    "                min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))\n",
    "            return min(max(min_pos, xmin_bound), xmax_bound)\n",
    "        else:\n",
    "            return (xmin_bound + xmax_bound) / 2.\n",
    "\n",
    "    def _strong_wolfe(self, obj_func, x, t, d, f, g, gtd, c1=1e-4, c2=0.9, tolerance_change=1e-9, max_ls=30): \n",
    "        d_norm = d.abs().max()\n",
    "        g = g.clone(memory_format=torch.contiguous_format)\n",
    "        f_new, g_new = obj_func(x, t, d)\n",
    "        ls_func_evals = 1\n",
    "        gtd_new = g_new.dot(d)\n",
    "        \n",
    "        if math.isinf(f_new) or math.isnan(f_new) or torch.isinf(gtd_new) or torch.isnan(gtd_new): \n",
    "            self.large_step_size_index.append(self.hes_iter)\n",
    "            t = t / g.abs().sum()\n",
    "            f_new, g_new = obj_func(x, t, d)\n",
    "            gtd_new = g_new.dot(d)\n",
    "\n",
    "        t_prev, f_prev, g_prev, gtd_prev = 0, f, g, gtd\n",
    "        done = False\n",
    "        ls_iter = 0\n",
    "        while ls_iter < max_ls:\n",
    "            if f_new > (f + c1 * t * gtd) or (ls_iter > 1 and f_new >= f_prev):\n",
    "                bracket = [t_prev, t]\n",
    "                bracket_f = [f_prev, f_new]\n",
    "                bracket_g = [g_prev, g_new.clone(memory_format=torch.contiguous_format)]\n",
    "                bracket_gtd = [gtd_prev, gtd_new]\n",
    "                break\n",
    "\n",
    "            if abs(gtd_new) <= -c2 * gtd:\n",
    "                bracket = [t]\n",
    "                bracket_f = [f_new]\n",
    "                bracket_g = [g_new]\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            if gtd_new >= 0:\n",
    "                bracket = [t_prev, t]\n",
    "                bracket_f = [f_prev, f_new]\n",
    "                bracket_g = [g_prev, g_new.clone(memory_format=torch.contiguous_format)]\n",
    "                bracket_gtd = [gtd_prev, gtd_new]\n",
    "                break\n",
    "\n",
    "            min_step = t + 0.01 * (t - t_prev)\n",
    "            max_step = t * 10\n",
    "            tmp = t\n",
    "            t = self._cubic_interpolate(\n",
    "                t_prev,\n",
    "                f_prev,\n",
    "                gtd_prev,\n",
    "                t,\n",
    "                f_new,\n",
    "                gtd_new,\n",
    "                bounds=(min_step, max_step))\n",
    "\n",
    "            t_prev = tmp\n",
    "            f_prev = f_new\n",
    "            g_prev = g_new.clone(memory_format=torch.contiguous_format)\n",
    "            gtd_prev = gtd_new\n",
    "            f_new, g_new = obj_func(x, t, d)\n",
    "            ls_func_evals += 1\n",
    "            gtd_new = g_new.dot(d)\n",
    "            ls_iter += 1\n",
    "\n",
    "        if ls_iter == max_ls:\n",
    "            bracket = [0, t]\n",
    "            bracket_f = [f, f_new]\n",
    "            bracket_g = [g, g_new]\n",
    "\n",
    "        insuf_progress = False\n",
    "        low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)\n",
    "        while not done and ls_iter < max_ls:\n",
    "            if abs(bracket[1] - bracket[0]) * d_norm < tolerance_change: break\n",
    "\n",
    "            t = self._cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0],\n",
    "                               bracket[1], bracket_f[1], bracket_gtd[1])\n",
    "\n",
    "            eps = 0.1 * (max(bracket) - min(bracket))\n",
    "            if min(max(bracket) - t, t - min(bracket)) < eps:\n",
    "                # interpolation close to boundary\n",
    "                if insuf_progress or t >= max(bracket) or t <= min(bracket):\n",
    "                    # evaluate at 0.1 away from boundary\n",
    "                    if abs(t - max(bracket)) < abs(t - min(bracket)):\n",
    "                        t = max(bracket) - eps\n",
    "                    else:\n",
    "                        t = min(bracket) + eps\n",
    "                    insuf_progress = False\n",
    "                else:\n",
    "                    insuf_progress = True\n",
    "            else:\n",
    "                insuf_progress = False\n",
    "\n",
    "            f_new, g_new = obj_func(x, t, d)\n",
    "            ls_func_evals += 1\n",
    "            gtd_new = g_new.dot(d)\n",
    "            ls_iter += 1\n",
    "\n",
    "            if f_new > (f + c1 * t * gtd) or f_new >= bracket_f[low_pos]:\n",
    "                bracket[high_pos] = t\n",
    "                bracket_f[high_pos] = f_new\n",
    "                bracket_g[high_pos] = g_new.clone(memory_format=torch.contiguous_format)\n",
    "                bracket_gtd[high_pos] = gtd_new\n",
    "                low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)\n",
    "            else:\n",
    "                if abs(gtd_new) <= -c2 * gtd:\n",
    "                    done = True\n",
    "                elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:\n",
    "                    # old high becomes new low\n",
    "                    bracket[high_pos] = bracket[low_pos]\n",
    "                    bracket_f[high_pos] = bracket_f[low_pos]\n",
    "                    bracket_g[high_pos] = bracket_g[low_pos]\n",
    "                    bracket_gtd[high_pos] = bracket_gtd[low_pos]\n",
    "\n",
    "                # new point becomes new low\n",
    "                bracket[low_pos] = t\n",
    "                bracket_f[low_pos] = f_new\n",
    "                bracket_g[low_pos] = g_new.clone(memory_format=torch.contiguous_format)\n",
    "                bracket_gtd[low_pos] = gtd_new\n",
    "\n",
    "        t = bracket[low_pos]\n",
    "        f_new = bracket_f[low_pos]\n",
    "        g_new = bracket_g[low_pos]\n",
    "        \n",
    "        return t\n",
    "\n",
    "def hvp(grad_params, params, v):\n",
    "    Hv = torch.autograd.grad(grad_params, params, grad_outputs = v,\n",
    "                              retain_graph = True)\n",
    "    Hv = tuple(Hvi.detach() for Hvi in Hv)\n",
    "    return torch.cat([Hvi.reshape(-1) for Hvi in Hv])\n",
    "\n",
    "def group_product(xs, ys):\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalize(v):\n",
    "    s = torch.sqrt(group_product(v, v))\n",
    "    v = [x / (s + 1e-6) for x in v]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960da319-534c-4d6d-8fcd-80fc8b0eb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(LSQ, self).__init__()\n",
    "        self.w = torch.nn.Linear(n_features, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96373c91-b880-47e1-a7f2-e7145b77595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Hessian: [[ 0.98961967  0.00881748 -0.0118267  ... -0.01783493  0.00830881\n",
      "   0.01661189]\n",
      " [ 0.00881748  1.01844719  0.0044552  ...  0.00497486 -0.00325788\n",
      "  -0.01623477]\n",
      " [-0.0118267   0.0044552   1.03757619 ... -0.00380699  0.00220931\n",
      "   0.00585819]\n",
      " ...\n",
      " [-0.01783493  0.00497486 -0.00380699 ...  1.02608195  0.00346571\n",
      "  -0.0090292 ]\n",
      " [ 0.00830881 -0.00325788  0.00220931 ...  0.00346571  0.95896859\n",
      "  -0.02342711]\n",
      " [ 0.01661189 -0.01623477  0.00585819 ... -0.0090292  -0.02342711\n",
      "   0.99756846]]\n"
     ]
    }
   ],
   "source": [
    "# define experiment parameters\n",
    "n_train = 5000\n",
    "n_test = 500\n",
    "n_features = 100\n",
    "n_iters = 50\n",
    "\n",
    "weight = np.random.normal(size=n_features)\n",
    "\n",
    "Xtrain = np.random.normal(size = (n_train, n_features))\n",
    "ytrain = (Xtrain @ weight)[: , np.newaxis]\n",
    "\n",
    "Xtest = np.sort(np.random.normal(size = (n_test, n_features)))\n",
    "ytest = (Xtest @ weight)[: , np.newaxis]\n",
    "\n",
    "print(f'True Hessian: {Xtrain.T @ Xtrain / n_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5bf22f3-ca4f-4be2-8fdf-bb80fffe12d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian Approximation: tensor([[ 0.9888,  0.0089, -0.0130,  ..., -0.0182,  0.0089,  0.0170],\n",
      "        [ 0.0089,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0163],\n",
      "        [-0.0130,  0.0045,  1.0359,  ..., -0.0043,  0.0031,  0.0064],\n",
      "        ...,\n",
      "        [-0.0182,  0.0050, -0.0043,  ...,  1.0259,  0.0037, -0.0088],\n",
      "        [ 0.0089, -0.0033,  0.0031,  ...,  0.0037,  0.9585, -0.0237],\n",
      "        [ 0.0170, -0.0163,  0.0064,  ..., -0.0088, -0.0237,  0.9974]])\n",
      "Hessian Approximation: tensor([[ 0.9892,  0.0089, -0.0114,  ..., -0.0175,  0.0081,  0.0161],\n",
      "        [ 0.0089,  1.0184,  0.0044,  ...,  0.0049, -0.0032, -0.0161],\n",
      "        [-0.0114,  0.0044,  1.0372,  ..., -0.0041,  0.0024,  0.0063],\n",
      "        ...,\n",
      "        [-0.0175,  0.0049, -0.0041,  ...,  1.0258,  0.0036, -0.0086],\n",
      "        [ 0.0081, -0.0032,  0.0024,  ...,  0.0036,  0.9589, -0.0237],\n",
      "        [ 0.0161, -0.0161,  0.0063,  ..., -0.0086, -0.0237,  0.9969]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0089, -0.0118,  ..., -0.0179,  0.0083,  0.0166],\n",
      "        [ 0.0089,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0163],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0179,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0163,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0032, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0058,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0119,  ..., -0.0177,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0119,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0177,  0.0050, -0.0038,  ...,  1.0259,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9589, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9891,  0.0084, -0.0115,  ..., -0.0173,  0.0079,  0.0165],\n",
      "        [ 0.0084,  1.0180,  0.0048,  ...,  0.0055, -0.0037, -0.0163],\n",
      "        [-0.0115,  0.0048,  1.0373,  ..., -0.0042,  0.0025,  0.0059],\n",
      "        ...,\n",
      "        [-0.0173,  0.0055, -0.0042,  ...,  1.0254,  0.0040, -0.0089],\n",
      "        [ 0.0079, -0.0037,  0.0025,  ...,  0.0040,  0.9586, -0.0235],\n",
      "        [ 0.0165, -0.0163,  0.0059,  ..., -0.0089, -0.0235,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9805,  0.0022, -0.0103,  ..., -0.0155, -0.0012,  0.0174],\n",
      "        [ 0.0022,  1.0136,  0.0056,  ...,  0.0067, -0.0102, -0.0156],\n",
      "        [-0.0103,  0.0056,  1.0373,  ..., -0.0042,  0.0038,  0.0057],\n",
      "        ...,\n",
      "        [-0.0155,  0.0067, -0.0042,  ...,  1.0255,  0.0059, -0.0092],\n",
      "        [-0.0012, -0.0102,  0.0038,  ...,  0.0059,  0.9491, -0.0226],\n",
      "        [ 0.0174, -0.0156,  0.0057,  ..., -0.0092, -0.0226,  0.9975]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0058,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9897,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0044,  ...,  0.0050, -0.0032, -0.0162],\n",
      "        [-0.0118,  0.0044,  1.0376,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0058,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0378,  ..., -0.0039,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0039,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0044,  ...,  0.0050, -0.0032, -0.0163],\n",
      "        [-0.0118,  0.0044,  1.0375,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0163,  0.0058,  ..., -0.0090, -0.0234,  0.9975]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 9.8913e-01,  9.4396e-03, -1.0703e-02,  ..., -1.8871e-02,\n",
      "          8.8680e-03,  1.7530e-02],\n",
      "        [ 9.4396e-03,  1.0177e+00,  3.0291e-03,  ...,  6.2900e-03,\n",
      "         -3.9671e-03, -1.7403e-02],\n",
      "        [-1.0703e-02,  3.0291e-03,  1.0350e+00,  ..., -1.4258e-03,\n",
      "          9.2455e-04,  3.7437e-03],\n",
      "        ...,\n",
      "        [-1.8871e-02,  6.2900e-03, -1.4258e-03,  ...,  1.0239e+00,\n",
      "          4.6505e-03, -7.0792e-03],\n",
      "        [ 8.8680e-03, -3.9671e-03,  9.2454e-04,  ...,  4.6505e-03,\n",
      "          9.5833e-01, -2.4479e-02],\n",
      "        [ 1.7530e-02, -1.7403e-02,  3.7437e-03,  ..., -7.0792e-03,\n",
      "         -2.4479e-02,  9.9584e-01]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0034, -0.0091],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0034,  0.9590, -0.0233],\n",
      "        [ 0.0166, -0.0162,  0.0058,  ..., -0.0091, -0.0233,  0.9978]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0044,  ...,  0.0049, -0.0032, -0.0162],\n",
      "        [-0.0118,  0.0044,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0049, -0.0038,  ...,  1.0260,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0032, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0058,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0377,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0260,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9589, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9897,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0032, -0.0163],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0091],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0163,  0.0058,  ..., -0.0091, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0044,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0044,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0044,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0044,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9898,  0.0089, -0.0119,  ..., -0.0175,  0.0083,  0.0166],\n",
      "        [ 0.0089,  1.0185,  0.0044,  ...,  0.0052, -0.0033, -0.0162],\n",
      "        [-0.0119,  0.0044,  1.0376,  ..., -0.0040,  0.0022,  0.0058],\n",
      "        ...,\n",
      "        [-0.0175,  0.0052, -0.0040,  ...,  1.0268,  0.0034, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0034,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0058,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9895,  0.0089, -0.0118,  ..., -0.0178,  0.0083,  0.0167],\n",
      "        [ 0.0089,  1.0184,  0.0045,  ...,  0.0049, -0.0032, -0.0163],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0049, -0.0038,  ...,  1.0261,  0.0035, -0.0091],\n",
      "        [ 0.0083, -0.0032,  0.0022,  ...,  0.0035,  0.9589, -0.0234],\n",
      "        [ 0.0167, -0.0163,  0.0059,  ..., -0.0091, -0.0234,  0.9975]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0034, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0034,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 1.0155e+00,  2.4854e-02,  2.6965e-03,  ..., -5.2348e-03,\n",
      "         -4.1489e-03,  3.6506e-02],\n",
      "        [ 2.4854e-02,  1.0284e+00,  1.3438e-02,  ...,  1.2768e-02,\n",
      "         -1.0962e-02, -3.9269e-03],\n",
      "        [ 2.6965e-03,  1.3438e-02,  1.0457e+00,  ...,  3.2519e-03,\n",
      "         -4.7665e-03,  1.7003e-02],\n",
      "        ...,\n",
      "        [-5.2348e-03,  1.2768e-02,  3.2519e-03,  ...,  1.0322e+00,\n",
      "         -2.5875e-03,  6.4155e-04],\n",
      "        [-4.1489e-03, -1.0962e-02, -4.7665e-03,  ..., -2.5875e-03,\n",
      "          9.6496e-01, -3.2988e-02],\n",
      "        [ 3.6506e-02, -3.9269e-03,  1.7003e-02,  ...,  6.4156e-04,\n",
      "         -3.2988e-02,  1.0128e+00]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0044,  ...,  0.0050, -0.0033, -0.0163],\n",
      "        [-0.0118,  0.0044,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0091],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0163,  0.0059,  ..., -0.0091, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0184,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9589, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0088, -0.0118,  ..., -0.0178,  0.0083,  0.0166],\n",
      "        [ 0.0088,  1.0185,  0.0045,  ...,  0.0050, -0.0033, -0.0162],\n",
      "        [-0.0118,  0.0045,  1.0376,  ..., -0.0038,  0.0022,  0.0059],\n",
      "        ...,\n",
      "        [-0.0178,  0.0050, -0.0038,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0022,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0162,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n",
      "Hessian Approximation: tensor([[ 0.9896,  0.0089, -0.0120,  ..., -0.0179,  0.0083,  0.0166],\n",
      "        [ 0.0089,  1.0184,  0.0046,  ...,  0.0050, -0.0033, -0.0163],\n",
      "        [-0.0120,  0.0046,  1.0372,  ..., -0.0039,  0.0023,  0.0059],\n",
      "        ...,\n",
      "        [-0.0179,  0.0050, -0.0039,  ...,  1.0261,  0.0035, -0.0090],\n",
      "        [ 0.0083, -0.0033,  0.0023,  ...,  0.0035,  0.9590, -0.0234],\n",
      "        [ 0.0166, -0.0163,  0.0059,  ..., -0.0090, -0.0234,  0.9976]])\n"
     ]
    }
   ],
   "source": [
    "model = LSQ(n_features)\n",
    "\n",
    "# specify optimizer\n",
    "# optimizer = torch.optim.LBFGS(model.parameters(), line_search_fn='strong_wolfe', lr=1.0)\n",
    "optimizer = SketchySGD(model.parameters(), lr=1.0, rank=100, rho=1e-3, chunk_size=5, hes_update_freq=1, momentum=0.0, line_search_fn='strong_wolfe', verbose=True)\n",
    "\n",
    "loss_hist = []\n",
    "step_size_hist = []\n",
    "\n",
    "Xt = torch.tensor(Xtrain, dtype=torch.float)\n",
    "yt = torch.tensor(ytrain, dtype=torch.float)\n",
    "\n",
    "torch.nn.init.zeros_(model.w.weight)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    model.train()\n",
    "    \n",
    "    def closure(): \n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xt)\n",
    "        loss = 0.5 * loss_function(output, yt)\n",
    "        if isinstance(optimizer, SketchySGD): \n",
    "            grad_tuple = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "            return loss, grad_tuple   \n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # record step size taken (if using linear search)\n",
    "    cur_step_size = None\n",
    "    if isinstance(optimizer, SketchySGD) and optimizer.state_dict()['param_groups'][0]['line_search_fn'] is not None: \n",
    "        cur_step_size = optimizer.state_dict()['state'][0]['step_size']\n",
    "    if isinstance(optimizer, torch.optim.LBFGS) and optimizer.state_dict()['param_groups'][0]['line_search_fn'] is not None: \n",
    "        cur_step_size = optimizer.state_dict()['state'][0]['t']\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(Xt)\n",
    "    loss = 0.5 * loss_function(output, yt).item()\n",
    "    loss_hist.append(loss)\n",
    "    if cur_step_size is not None: \n",
    "        step_size_hist.append(cur_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94449e35-1e7a-4a43-ad45-8f3e9daa0da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.632312328861632e-13\n"
     ]
    }
   ],
   "source": [
    "# compute test loss\n",
    "with torch.no_grad():\n",
    "    Xttest = torch.tensor(Xtest, dtype=torch.float)\n",
    "    yttest = torch.tensor(ytest, dtype=torch.float)\n",
    "    output = model(Xttest)\n",
    "    loss =  0.5 * loss_function(output, yttest)\n",
    "print(f'Test Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55d11de8-7edc-4bdd-b0a6-1ee631836df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Least Squares / SketchySGD')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/pdf": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make plot\n",
    "set_matplotlib_formats('pdf')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot()\n",
    "step_line = ax1.plot([float(i) for i in step_size_hist], label='step size', color='C4', alpha=0.85, marker='o', markersize=2, linestyle='solid', linewidth=0.35)\n",
    "ax1.set_ylabel('step size')\n",
    "ax1.set_xlabel('iteration')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "loss_line = ax2.semilogy(loss_hist, label='loss', alpha=0.5)\n",
    "ax2.set_ylabel('loss')\n",
    "\n",
    "lines = step_line + loss_line\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "optimizer_name = \"SketchySGD\" if isinstance(optimizer, SketchySGD) else \"L-BFGS\"\n",
    "ax1.set_title(f'Least Squares / {optimizer_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af4fd8f-a55e-4340-9858-aad3e4618d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49.9944953918457,\n",
       " 0.00011550551425898448,\n",
       " 2.5517304935718244e-10,\n",
       " 4.060598428681783e-13,\n",
       " 3.969303184747641e-13,\n",
       " 3.968911787763374e-13,\n",
       " 3.9634615034422893e-13,\n",
       " 3.954164469813226e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13,\n",
       " 3.92796960322489e-13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out loss history\n",
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4d70e-8562-48da-9532-3eb935373c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
