{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.func import vmap\n",
    "from functools import reduce\n",
    "\n",
    "def _armijo(f, x, gx, dx, t, alpha=0.1, beta=0.5):\n",
    "    f0 = f(x, 0, dx)\n",
    "    f1 = f(x, t, dx)\n",
    "    while f1 > f0 + alpha * t * gx.dot(dx):\n",
    "        t *= beta\n",
    "        f1 = f(x, t, dx)\n",
    "    return t\n",
    "\n",
    "\n",
    "def _apply_nys_precond_inv(U, S_mu_inv, mu, lambd_r, x):\n",
    "    z = U.T @ x\n",
    "    z = (lambd_r + mu) * (U @ (S_mu_inv * z)) + (x - U @ z)\n",
    "    return z\n",
    "\n",
    "\n",
    "def _nystrom_pcg(hess, b, x, mu, U, S, r, tol, max_iters):\n",
    "    lambd_r = S[r - 1]\n",
    "    S_mu_inv = (S + mu) ** (-1)\n",
    "\n",
    "    resid = b - (hess(x) + mu * x)\n",
    "    z = _apply_nys_precond_inv(U, S_mu_inv, mu, lambd_r, resid)\n",
    "    p = z.clone()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while torch.norm(resid) > tol and i < max_iters:\n",
    "        v = hess(p) + mu * p\n",
    "        alpha = torch.dot(resid, z) / torch.dot(p, v)\n",
    "        x += alpha * p\n",
    "\n",
    "        rTz = torch.dot(resid, z)\n",
    "        resid -= alpha * v\n",
    "        z = _apply_nys_precond_inv(U, S_mu_inv, mu, lambd_r, resid)\n",
    "        beta = torch.dot(resid, z) / rTz\n",
    "\n",
    "        p = z + beta * p\n",
    "        i += 1\n",
    "\n",
    "    if torch.norm(resid) > tol:\n",
    "        print(\"Warning: PCG did not converge to tolerance\")\n",
    "\n",
    "    return x\n",
    "\n",
    "class NysNewtonCG(Optimizer):\n",
    "    def __init__(self, params, lr=1.0, rank=10, mu=1e-4, chunk_size=1,\n",
    "                  cg_tol=1e-10, cg_max_iters=100, line_search_fn=None, verbose=False):\n",
    "        defaults = dict(lr=lr, rank=rank, chunk_size=chunk_size, mu=mu, cg_tol=cg_tol,\n",
    "                        cg_max_iters=cg_max_iters, line_search_fn=line_search_fn)\n",
    "        self.rank = rank\n",
    "        self.mu = mu\n",
    "        self.chunk_size = chunk_size\n",
    "        self.cg_tol = cg_tol\n",
    "        self.cg_max_iters = cg_max_iters\n",
    "        self.line_search_fn = line_search_fn\n",
    "        self.verbose = verbose\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        self.n_iters = 0\n",
    "        super(NysNewtonCG, self).__init__(params, defaults)\n",
    "\n",
    "        if len(self.param_groups) > 1:\n",
    "            raise ValueError(\n",
    "                \"NysNewtonCG doesn't currently support per-parameter options (parameter groups)\")\n",
    "\n",
    "        if self.line_search_fn is not None and self.line_search_fn != 'armijo':\n",
    "            raise ValueError(\"NysNewtonCG only supports Armijo line search\")\n",
    "\n",
    "        self._params = self.param_groups[0]['params']\n",
    "        self._params_list = list(self._params)\n",
    "        self._numel_cache = None\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if self.n_iters == 0:\n",
    "            # Store the previous direction for warm starting PCG\n",
    "            self.old_dir = torch.zeros(\n",
    "                self._numel(), device=self._params[0].device)\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        g = torch.cat([p.grad.view(-1)\n",
    "                      for group in self.param_groups for p in group['params'] if p.grad is not None])\n",
    "        # g = g.detach()\n",
    "\n",
    "        # one step update\n",
    "        for group_idx, group in enumerate(self.param_groups):\n",
    "            # Calculate the Newton direction\n",
    "            d = _nystrom_pcg(lambda x: self._hvp(g, self._params_list, x), g, self.old_dir,\n",
    "                             self.mu, self.U, self.S, self.rank, self.cg_tol, self.cg_max_iters)\n",
    "\n",
    "            # Store the previous direction for warm starting PCG\n",
    "            self.old_dir = d\n",
    "\n",
    "            # Check if d is a descent direction\n",
    "            if torch.dot(d, g) <= 0:\n",
    "                print(\"Warning: d is not a descent direction\")\n",
    "\n",
    "            if self.line_search_fn == 'armijo':\n",
    "                x_init = self._clone_param()\n",
    "\n",
    "                def obj_func(x, t, dx):\n",
    "                    self._add_grad(t, dx)\n",
    "                    loss = float(closure())\n",
    "                    self._set_param(x)\n",
    "                    return loss\n",
    "\n",
    "                # Use -d for convention\n",
    "                t = _armijo(obj_func, x_init, g, -d, group['lr'])\n",
    "            else:\n",
    "                t = group['lr']\n",
    "\n",
    "            self.state[group_idx]['t'] = t\n",
    "\n",
    "            # update parameters\n",
    "            ls = 0\n",
    "            for p in group['params']:\n",
    "                np = torch.numel(p)\n",
    "                dp = d[ls:ls+np].view(p.shape)\n",
    "                ls += np\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data.add_(-dp, alpha=t)\n",
    "\n",
    "        self.n_iters += 1\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_preconditioner(self, grad_tuple):\n",
    "        gradsH = torch.cat([gradient.view(-1)\n",
    "                           for gradient in grad_tuple if gradient is not None])\n",
    "\n",
    "        p = gradsH.shape[0]\n",
    "        # Generate test matrix (NOTE: This is transposed test matrix)\n",
    "        Phi = torch.randn(\n",
    "            (self.rank, p), device=self._params_list[0].device) / (p ** 0.5)\n",
    "        Phi = torch.linalg.qr(Phi.t(), mode='reduced')[0].t()\n",
    "\n",
    "        Y = self._hvp_vmap(gradsH, self._params_list)(Phi)\n",
    "\n",
    "        # Calculate shift\n",
    "        shift = torch.finfo(Y.dtype).eps\n",
    "        Y_shifted = Y + shift * Phi\n",
    "        # Calculate Phi^T * H * Phi (w/ shift) for Cholesky\n",
    "        choleskytarget = torch.mm(Y_shifted, Phi.t())\n",
    "        # Perform Cholesky, if fails, do eigendecomposition\n",
    "        # The new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T\n",
    "            C = torch.linalg.cholesky(\n",
    "                torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        try:\n",
    "            B = torch.linalg.solve_triangular(\n",
    "                C, Y_shifted, upper=False, left=True)\n",
    "        # temporary fix for issue @ https://github.com/pytorch/pytorch/issues/97211\n",
    "        except:\n",
    "            B = torch.linalg.solve_triangular(C.to('cpu'), Y_shifted.to(\n",
    "                'cpu'), upper=False, left=True).to(C.device)\n",
    "        # B = V * S * U^T b/c we have been using transposed sketch\n",
    "        _, S, UT = torch.linalg.svd(B, full_matrices=False)\n",
    "        self.U = UT.t()\n",
    "        self.S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "        self.rho = self.S[-1]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Approximate eigenvalues = {self.S}')\n",
    "\n",
    "    def _hvp_vmap(self, grad_params, params):\n",
    "        return vmap(lambda v: self._hvp(grad_params, params, v), in_dims=0, chunk_size=self.chunk_size)\n",
    "\n",
    "    def _hvp(self, grad_params, params, v):\n",
    "        Hv = torch.autograd.grad(grad_params, params, grad_outputs=v,\n",
    "                                 retain_graph=True)\n",
    "        Hv = tuple(Hvi.detach() for Hvi in Hv)\n",
    "        return torch.cat([Hvi.reshape(-1) for Hvi in Hv])\n",
    "\n",
    "    def _numel(self):\n",
    "        if self._numel_cache is None:\n",
    "            self._numel_cache = reduce(\n",
    "                lambda total, p: total + p.numel(), self._params, 0)\n",
    "        return self._numel_cache\n",
    "\n",
    "    def _add_grad(self, step_size, update):\n",
    "        offset = 0\n",
    "        for p in self._params:\n",
    "            numel = p.numel()\n",
    "            # Avoid in-place operation by creating a new tensor\n",
    "            p.data = p.data.add(\n",
    "                update[offset:offset + numel].view_as(p), alpha=step_size)\n",
    "            offset += numel\n",
    "        assert offset == self._numel()\n",
    "\n",
    "    def _clone_param(self):\n",
    "        return [p.clone(memory_format=torch.contiguous_format) for p in self._params]\n",
    "\n",
    "    def _set_param(self, params_data):\n",
    "        for p, pdata in zip(self._params, params_data):\n",
    "            # Replace the .data attribute of the tensor\n",
    "            p.data = pdata.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ(torch.nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LSQ, self).__init__()\n",
    "        self.w = torch.nn.Linear(n_features, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Hessian: [[ 5.68452123e-07  6.14098800e-07  2.08028781e-07 ...  9.04738043e-08\n",
      "   4.54935270e-07  3.13811714e-07]\n",
      " [ 6.14098800e-07  9.26286418e-06  5.69873508e-07 ...  2.16071361e-06\n",
      "   5.48718709e-06  1.68008852e-06]\n",
      " [ 2.08028781e-07  5.69873508e-07  1.01541385e-06 ...  2.54008324e-07\n",
      "  -2.18441201e-07  6.78250338e-07]\n",
      " ...\n",
      " [ 9.04738043e-08  2.16071361e-06  2.54008324e-07 ...  1.43845159e-06\n",
      "   2.43443285e-06  5.92922560e-07]\n",
      " [ 4.54935270e-07  5.48718709e-06 -2.18441201e-07 ...  2.43443285e-06\n",
      "   7.95394144e-06  3.52828111e-07]\n",
      " [ 3.13811714e-07  1.68008852e-06  6.78250338e-07 ...  5.92922560e-07\n",
      "   3.52828111e-07  1.57083166e-06]]\n"
     ]
    }
   ],
   "source": [
    "# define experiment parameters\n",
    "n_train = 5000\n",
    "n_test = 500\n",
    "n_features = 100\n",
    "n_iters = 20\n",
    "\n",
    "weight = np.random.normal(size=n_features)\n",
    "\n",
    "# Xtrain = np.random.normal(size = (n_train, n_features))\n",
    "# ytrain = (Xtrain @ weight)[: , np.newaxis]\n",
    "\n",
    "# Xtest = np.sort(np.random.normal(size = (n_test, n_features)))\n",
    "# ytest = (Xtest @ weight)[: , np.newaxis]\n",
    "\n",
    "# Create a vector with polynomial decay starting at 1\n",
    "decay = (np.arange(n_features) + 1) ** (-1.0)\n",
    "decay = np.diag(decay)\n",
    "U = ortho_group.rvs(n_train)[:, :n_features]\n",
    "VT = ortho_group.rvs(n_features)\n",
    "X = U @ decay @ VT\n",
    "\n",
    "Xtrain = X[:n_train]\n",
    "ytrain = (Xtrain @ weight)[:, np.newaxis]\n",
    "\n",
    "# Xtest = X[n_train:]\n",
    "# ytest = (Xtest @ weight)[: , np.newaxis]\n",
    "\n",
    "print(f'True Hessian: {Xtrain.T @ Xtrain / n_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Loss: 5.350874416812498e-15\n",
      "Iteration 1 | Loss: 2.4241024159234008e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 | Loss: 2.6956726352646133e-18\n",
      "Iteration 3 | Loss: 1.7643938241636248e-18\n",
      "Iteration 4 | Loss: 1.732391447034867e-18\n",
      "Iteration 5 | Loss: 1.6941064263586158e-18\n",
      "Iteration 6 | Loss: 1.6941064263586158e-18\n",
      "Iteration 7 | Loss: 1.6941064263586158e-18\n",
      "Iteration 8 | Loss: 1.6941064263586158e-18\n",
      "Iteration 9 | Loss: 1.6941064263586158e-18\n",
      "Iteration 10 | Loss: 1.6941064263586158e-18\n",
      "Iteration 11 | Loss: 1.6941064263586158e-18\n",
      "Iteration 12 | Loss: 1.6941064263586158e-18\n",
      "Iteration 13 | Loss: 1.6941064263586158e-18\n",
      "Iteration 14 | Loss: 1.6941064263586158e-18\n",
      "Iteration 15 | Loss: 1.6941064263586158e-18\n",
      "Iteration 16 | Loss: 1.6941064263586158e-18\n",
      "Iteration 17 | Loss: 1.6941064263586158e-18\n",
      "Iteration 18 | Loss: 1.6941064263586158e-18\n",
      "Iteration 19 | Loss: 1.6941064263586158e-18\n"
     ]
    }
   ],
   "source": [
    "model = LSQ(n_features)\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = NysNewtonCG(model.parameters(), lr=1.0,\n",
    "                      rank=10, mu=0, cg_tol=1e-16, cg_max_iters=1000, line_search_fn='armijo')\n",
    "precond_update_freq = 20\n",
    "\n",
    "loss_hist = []\n",
    "step_size_hist = []\n",
    "\n",
    "Xt = torch.tensor(Xtrain, dtype=torch.float)\n",
    "yt = torch.tensor(ytrain, dtype=torch.float)\n",
    "\n",
    "torch.nn.init.zeros_(model.w.weight)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    model.train()\n",
    "\n",
    "    # Update preconditioner for PCG\n",
    "    if i % precond_update_freq == 0:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xt)\n",
    "        loss = loss_function(output, yt)\n",
    "        grad_tuple = torch.autograd.grad(\n",
    "            loss, model.parameters(), create_graph=True)\n",
    "        optimizer.update_preconditioner(grad_tuple)\n",
    "\n",
    "    # Take a step\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(Xt)\n",
    "        loss = loss_function(output, yt)\n",
    "        loss.backward(create_graph=True)\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    model.eval()\n",
    "    output = model(Xt)\n",
    "    loss = loss_function(output, yt).item()\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    step_size_hist.append(optimizer.state_dict()['state'][0]['t'])\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f'Iteration {i} | Loss: {loss}')\n",
    "        # print(optimizer.U @ torch.diag(optimizer.S) @ optimizer.U.t())\n",
    "        # print(optimizer.S)\n",
    "        # print(optimizer.rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_for_pinns_env",
   "language": "python",
   "name": "opt_for_pinns_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
