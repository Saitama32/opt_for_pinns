{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn as nn\n",
    "from torch.func import vmap\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LSQ, self).__init__()\n",
    "        self.w = torch.nn.Linear(n_features, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_features = 100\n",
    "\n",
    "weight = np.random.normal(size=n_features)\n",
    "\n",
    "Xtrain = np.random.normal(size = (n_train, n_features))\n",
    "ytrain = (Xtrain @ weight)[: , np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchySGD(Optimizer):\n",
    "    \"\"\"Implements SketchySGD. We assume that there is only one parameter group to optimize.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rank (int): sketch rank\n",
    "        rho (float): regularization\n",
    "        lr (float): learning rate\n",
    "        weight_decay (float): weight decay parameter\n",
    "        hes_update_freq (int): how frequently we should update the Hessian approximation\n",
    "        momentum (float): momentum parameter\n",
    "        proportional (bool): option to maintain lr to rho ratio, even when lr decays\n",
    "        chunk_size (int): number of Hessian-vector products to compute in parallel\n",
    "                          if set to None, binary search will be used to find the maximally allowed value\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1, lr = 0.01, weight_decay = 0.0,\n",
    "                 hes_update_freq = 100, momentum = 0.0, proportional = False, chunk_size = None,\n",
    "                 line_search_fn = None):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho, lr = lr, weight_decay = weight_decay, \n",
    "                        hes_update_freq = hes_update_freq, proportional = proportional,\n",
    "                        chunk_size = chunk_size, line_search_fn = line_search_fn)\n",
    "        self.rank = rank\n",
    "        self.hes_update_freq = hes_update_freq\n",
    "        self.proportional = proportional\n",
    "        self.chunk_size = chunk_size\n",
    "        self.line_search_fn = line_search_fn\n",
    "        self.ratio = rho / lr\n",
    "        self.hes_iter = 0\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        self.counter = 0\n",
    "        self.momentum = momentum\n",
    "        self.momentum_buffer = None\n",
    "        super(SketchySGD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        grad_tuple = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss, grad_tuple = closure()\n",
    "\n",
    "        # update Hessian approximation, if needed\n",
    "        g = torch.cat([gradient.view(-1) for gradient in grad_tuple if gradient is not None])\n",
    "        if self.hes_iter % self.hes_update_freq == 0:\n",
    "            params = []\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "\n",
    "            # update preconditioner\n",
    "            self._update_preconditioner(params, g)\n",
    "\n",
    "        g = g.detach()\n",
    "\n",
    "        # update momentum buffer\n",
    "        if self.momentum_buffer is None: \n",
    "            self.momentum_buffer = g\n",
    "        else:\n",
    "            self.momentum_buffer = self.momentum * self.momentum_buffer + g\n",
    "\n",
    "        # get learning rate based on line search\n",
    "        if self.line_search_fn is not None: \n",
    "            if self.line_search_fn != 'backtracking':\n",
    "                raise ValueError(f'Line search function {self.line_search_fn} not supported.')\n",
    "            elif len(self.param_groups) != 1:\n",
    "                raise ValueError('Line search only supported for a single parameter group.')\n",
    "            elif self.momentum != 0.0:\n",
    "                raise ValueError('Line search not supported with momentum.')\n",
    "            # elif self.weight_decay != 0.0:\n",
    "            #     raise ValueError('Line search not supported with weight decay.')\n",
    "            \n",
    "            if self.line_search_fn == 'backtracking':\n",
    "                lr = group['lr']\n",
    "                # Adjust rho to be proportional to lr, if necessary\n",
    "                if self.proportional:\n",
    "                    rho = lr * self.ratio\n",
    "                else:\n",
    "                    rho = self.param_groups[0]['rho']\n",
    "\n",
    "                UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "                g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "                direction = -g_new\n",
    "\n",
    "                curr_params = self._clone_param(0)          \n",
    "                def obj_func(params_curr, search_dir, step_size):\n",
    "                    # get new parameters\n",
    "                    self._add_grad(0, step_size, search_dir)\n",
    "                    loss = float(closure()[0])\n",
    "\n",
    "                    # reset parameters\n",
    "                    with torch.no_grad():\n",
    "                        self._set_param(0, params_curr)\n",
    "\n",
    "                    return loss\n",
    "                \n",
    "                t = self._backtracking(obj_func, loss, curr_params, g, direction, lr)\n",
    "\n",
    "                print(f'Line search step size = {t}')\n",
    "\n",
    "                # update model parameters\n",
    "                self._add_grad(0, t, direction)\n",
    "\n",
    "        else:\n",
    "        # one step update\n",
    "            for group in self.param_groups:\n",
    "                lr = group['lr']\n",
    "                weight_decay = group['weight_decay']\n",
    "\n",
    "                # Adjust rho to be proportional to lr, if necessary\n",
    "                if self.proportional:\n",
    "                    rho = lr * self.ratio\n",
    "                else:\n",
    "                    rho = group['rho']\n",
    "\n",
    "                # compute gradient as a long vector\n",
    "                # g = torch.cat([p.grad.view(-1) for p in group['params'] if p.grad is not None]) # only get gradients if they exist!\n",
    "                # calculate the search direction by Nystrom sketch and solve\n",
    "                UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "                g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "                \n",
    "                ls = 0\n",
    "                # update model parameters\n",
    "                for p in group['params']:\n",
    "                    gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                    ls += torch.numel(p)\n",
    "                    p.data.add_(-lr * (gp + weight_decay * p.data)) # use weight decay (not same as L2 reg.)\n",
    "        \n",
    "        self.hes_iter += 1\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def _update_preconditioner(self, params, gradsH):\n",
    "        p = gradsH.shape[0]\n",
    "        # Generate test matrix (NOTE: This is transposed test matrix)\n",
    "        Phi = (torch.randn(self.rank, p) / (p ** 0.5)).to(params[0].device)\n",
    "        Phi = torch.linalg.qr(Phi.t(), mode = 'reduced')[0].t()\n",
    "\n",
    "        # Calculate sketch (NOTE: This is transposed sketch)\n",
    "        # Use binary search to find the maximally allowed chunk_size (only when chunk_size has not been set)\n",
    "        if self.chunk_size is None: \n",
    "            # start with the rank\n",
    "            self.chunk_size = self.rank\n",
    "            # set bounds for the search\n",
    "            max_size = self.rank\n",
    "            min_size = 1\n",
    "            while(True): \n",
    "                # update lower bound if attempted computation was successful\n",
    "                try: \n",
    "                    Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "                    min_size = self.chunk_size\n",
    "                    # search range has converged to a single point\n",
    "                    if max_size - min_size <= 1: \n",
    "                        margin_factor = 1.0\n",
    "                        # grab memory information\n",
    "                        free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "                        if free_mem / total_mem < 0.05: \n",
    "                            margin_factor = 0.95\n",
    "                        # create some safety margin (e.g. 95% of the found size)\n",
    "                        self.chunk_size = max(1, int(margin_factor * min_size))\n",
    "                        torch.cuda.empty_cache()\n",
    "                        break\n",
    "                # update upper bound if attempted computation ran out of memory\n",
    "                except RuntimeError as e:\n",
    "                    if str(e).startswith('CUDA out of memory.') and self.chunk_size > 1:\n",
    "                        max_size = self.chunk_size\n",
    "                        torch.cuda.empty_cache()\n",
    "                    # terminate if other runtime error occured or chunk_size = 1 still ran out of memory\n",
    "                    else: \n",
    "                        raise e\n",
    "                # halve the search range\n",
    "                self.chunk_size = int(0.5 * (min_size + max_size))\n",
    "            # report final chunk size\n",
    "            print(f'SketchySGD: chunk size has been set to {self.chunk_size}.')\n",
    "        # use previously set chunk size\n",
    "        else: \n",
    "            Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "            Y2 = self._hvp_vmap(gradsH, params)(torch.eye(p).to(params[0].device))\n",
    "            # print(f'True Hessian = {Y2}')\n",
    "\n",
    "        # Calculate shift\n",
    "        shift = torch.finfo(Y.dtype).eps\n",
    "        Y_shifted = Y + shift * Phi\n",
    "        # Calculate Phi^T * H * Phi (w/ shift) for Cholesky\n",
    "        choleskytarget = torch.mm(Y_shifted, Phi.t())\n",
    "        # Perform Cholesky, if fails, do eigendecomposition\n",
    "        # The new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        try: \n",
    "            B = torch.linalg.solve_triangular(C, Y_shifted, upper = False, left = True)\n",
    "        # temporary fix for issue @ https://github.com/pytorch/pytorch/issues/97211\n",
    "        except: \n",
    "            B = torch.linalg.solve_triangular(C.to('cpu'), Y_shifted.to('cpu'), upper = False, left = True).to(C.device)\n",
    "        _, S, UT = torch.linalg.svd(B, full_matrices = False) # B = V * S * U^T b/c we have been using transposed sketch\n",
    "        self.U = UT.t()\n",
    "        self.S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "        # print(f'Low-rank approximation (without rho) = {torch.mm(torch.mm(self.U, torch.diag(self.S)), self.U.t())}')\n",
    "\n",
    "    def _hvp_vmap(self, grad_params, params):\n",
    "        return vmap(lambda v: hvp(grad_params, params, v), in_dims = 0, chunk_size=self.chunk_size)\n",
    "    \n",
    "    def _clone_param(self, group_idx):\n",
    "        return [p.clone(memory_format=torch.contiguous_format) for p in self.param_groups[group_idx]['params']]\n",
    "    \n",
    "    def _add_grad(self, group_idx, step_size, update):\n",
    "        offset = 0\n",
    "        for p in self.param_groups[group_idx]['params']:\n",
    "            numel = p.numel()\n",
    "            p.data.add_(update[offset:offset + numel].view_as(p), alpha=step_size)\n",
    "            offset += numel\n",
    "\n",
    "    def _set_param(self, group_idx, params_data):\n",
    "        for p, pdata in zip(self.param_groups[group_idx]['params'], params_data):\n",
    "            p.copy_(pdata)\n",
    "    \n",
    "    # Write a backtracking line search that uses the closure as the loss function\n",
    "    # The line search function should take the following inputs: obj_func, loss_cur, parameters, gradient, search direction, initial step size, and backtracking parameters\n",
    "    # The line search function should return the appropriate step size\n",
    "    def _backtracking(self, obj_func, loss_cur, params, grad, search_dir, init_step_size, alpha=1e-4, beta=0.5):\n",
    "        # initialize step size\n",
    "        step_size = init_step_size\n",
    "        # evaluate loss at current parameters\n",
    "        loss_new = obj_func(params, search_dir, step_size)\n",
    "\n",
    "        # while loss at new parameters is greater than loss at current parameters plus sufficient decrease\n",
    "        while loss_new > loss_cur + alpha * step_size * torch.dot(grad, search_dir):\n",
    "            # update step size\n",
    "            step_size *= beta\n",
    "            # evaluate loss at new parameters\n",
    "            loss_new = obj_func(params, search_dir, step_size)\n",
    "\n",
    "        return step_size\n",
    "\n",
    "def hvp(grad_params, params, v):\n",
    "    Hv = torch.autograd.grad(grad_params, params, grad_outputs = v,\n",
    "                              retain_graph = True)\n",
    "    Hv = tuple(Hvi.detach() for Hvi in Hv)\n",
    "    return torch.cat([Hvi.reshape(-1) for Hvi in Hv])\n",
    "\n",
    "def group_product(xs, ys):\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalize(v):\n",
    "    s = torch.sqrt(group_product(v, v))\n",
    "    v = [x / (s + 1e-6) for x in v]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line search step size = 1.0\n",
      "loss: 0.0003976186562795192\n",
      "actual loss: 3.976186491172484e-08\n",
      "Line search step size = 1.0\n",
      "loss: 1.5470251746307895e-09\n",
      "actual loss: 1.5470251761486725e-13\n",
      "Line search step size = 1.0\n",
      "loss: 8.176378953235475e-13\n",
      "actual loss: 8.176379261343709e-17\n",
      "Line search step size = 1.0\n",
      "loss: 8.096685214648014e-13\n",
      "actual loss: 8.0966853724079e-17\n",
      "Line search step size = 1.0\n",
      "loss: 8.070943002466691e-13\n",
      "actual loss: 8.07094285000076e-17\n",
      "Line search step size = 1.0\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "Line search step size = 0.001953125\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n"
     ]
    }
   ],
   "source": [
    "model = LSQ(n_features)\n",
    "\n",
    "optimizer = SketchySGD(model.parameters(), lr=1e0, rank=100, rho=1e-3, \n",
    "                       chunk_size=5, hes_update_freq=1, momentum=0, line_search_fn='backtracking')\n",
    "\n",
    "loss_hist = []\n",
    "step_size_hist = []\n",
    "\n",
    "Xt = torch.tensor(Xtrain, dtype=torch.float)\n",
    "yt = torch.tensor(ytrain, dtype=torch.float)\n",
    "\n",
    "torch.nn.init.zeros_(model.w.weight)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "n_iters = 300\n",
    "\n",
    "for i in range(n_iters):\n",
    "    model.train()\n",
    "    def closure(): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(Xt)\n",
    "        loss = loss_function(output, yt)\n",
    "        if isinstance(optimizer, SketchySGD): \n",
    "            grad_tuple = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "            return loss, grad_tuple   \n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    # print(f\"new weights: {model.w.weight}\")\n",
    "    # cur_step_size = None\n",
    "    # if isinstance(optimizer, SketchySGD): \n",
    "    #     cur_step_size = optimizer.state_dict()['state'][0]['step_size']\n",
    "    # if isinstance(optimizer, torch.optim.LBFGS): \n",
    "    #     cur_step_size = optimizer.state_dict()['state'][0]['t']\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(Xt)\n",
    "    loss = loss_function(output, yt).item()\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print(f\"loss: {loss}\")\n",
    "    print(f\"actual loss: {0.5 * torch.mm(Xt, model.w.weight.T).sub(yt).pow(2).mean() / n_train}\")\n",
    "    # if cur_step_size is not None: \n",
    "    #     step_size_hist.append(cur_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024630.8334019077"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(ytrain - Xtrain @ weight)) / n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of loss function at 0 = [[-3.50980785]\n",
      " [-0.80856654]\n",
      " [-2.24949989]\n",
      " [-4.49936928]\n",
      " [-3.99599767]\n",
      " [ 1.84407324]\n",
      " [-1.68642628]\n",
      " [ 0.88081046]\n",
      " [ 0.63077351]\n",
      " [-0.80449884]\n",
      " [-0.10487917]\n",
      " [-2.94631201]\n",
      " [-1.61747569]\n",
      " [-0.89529129]\n",
      " [-0.75374459]\n",
      " [-0.63781025]\n",
      " [-3.19499123]\n",
      " [ 0.48344918]\n",
      " [-0.86577851]\n",
      " [ 2.11187996]\n",
      " [ 5.30796451]\n",
      " [-1.53338786]\n",
      " [-1.82679659]\n",
      " [ 1.12260823]\n",
      " [-4.51760161]\n",
      " [ 3.00177458]\n",
      " [ 0.21780347]\n",
      " [ 0.3146846 ]\n",
      " [-2.61681176]\n",
      " [-2.93555551]\n",
      " [-0.01832597]\n",
      " [-0.68875921]\n",
      " [ 1.88163691]\n",
      " [ 4.12919686]\n",
      " [ 0.74330581]\n",
      " [-0.62440497]\n",
      " [-2.33251757]\n",
      " [-2.23836417]\n",
      " [ 0.70230764]\n",
      " [ 0.82255751]\n",
      " [ 2.16085296]\n",
      " [ 2.66812308]\n",
      " [ 3.66481346]\n",
      " [-3.92013085]\n",
      " [ 0.88257894]\n",
      " [ 1.03856916]\n",
      " [ 2.7597351 ]\n",
      " [-1.22528283]\n",
      " [ 3.27964368]\n",
      " [ 0.26857325]\n",
      " [ 1.92192018]\n",
      " [-0.76638045]\n",
      " [ 0.98503741]\n",
      " [ 2.67978158]\n",
      " [-0.06389895]\n",
      " [-1.29241137]\n",
      " [-0.38702559]\n",
      " [-0.7419447 ]\n",
      " [ 1.7138443 ]\n",
      " [ 0.57477444]\n",
      " [ 1.31440639]\n",
      " [ 0.68825947]\n",
      " [ 1.66535874]\n",
      " [ 3.38836009]\n",
      " [-0.30809725]\n",
      " [ 0.69278672]\n",
      " [ 3.1230711 ]\n",
      " [-0.47557495]\n",
      " [ 1.89196825]\n",
      " [-0.12479198]\n",
      " [-1.47715306]\n",
      " [-0.41835683]\n",
      " [-2.19923745]\n",
      " [ 2.46181195]\n",
      " [-0.8787131 ]\n",
      " [ 1.5735617 ]\n",
      " [ 2.05716248]\n",
      " [ 1.04263968]\n",
      " [ 0.68362204]\n",
      " [-0.53186663]\n",
      " [ 1.84586227]\n",
      " [-1.80461916]\n",
      " [-1.07585534]\n",
      " [ 3.5449479 ]\n",
      " [-2.54145568]\n",
      " [-3.01414977]\n",
      " [-2.43480208]\n",
      " [ 0.61734338]\n",
      " [ 2.06150921]\n",
      " [-2.25685529]\n",
      " [ 1.29168954]\n",
      " [-2.51256821]\n",
      " [-0.55360514]\n",
      " [-2.01922311]\n",
      " [-0.51604391]\n",
      " [-1.14087268]\n",
      " [ 0.29329991]\n",
      " [-3.63903662]\n",
      " [-0.09888503]\n",
      " [-0.68510542]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"gradient of loss function at 0 = {2 * -Xtrain.T @ ytrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian of loss function = [[ 1.97923935  0.01763496 -0.02365339 ... -0.03566987  0.01661763\n",
      "   0.03322379]\n",
      " [ 0.01763496  2.03689437  0.00891041 ...  0.00994972 -0.00651575\n",
      "  -0.03246954]\n",
      " [-0.02365339  0.00891041  2.07515238 ... -0.00761399  0.00441863\n",
      "   0.01171637]\n",
      " ...\n",
      " [-0.03566987  0.00994972 -0.00761399 ...  2.05216389  0.00693142\n",
      "  -0.01805839]\n",
      " [ 0.01661763 -0.00651575  0.00441863 ...  0.00693142  1.91793718\n",
      "  -0.04685423]\n",
      " [ 0.03322379 -0.03246954  0.01171637 ... -0.01805839 -0.04685423\n",
      "   1.99513691]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"hessian of loss function = {2 * Xtrain.T @ Xtrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0003976186562795192, 1.5470251746307895e-09, 8.176378953235475e-13, 8.096685214648014e-13, 8.070943002466691e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot()\n",
    "step_line = ax1.plot([float(i) for i in step_size_hist], label='step size', color='C4', alpha=0.85, marker='o', markersize=2, linestyle='solid', linewidth=0.35)\n",
    "ax1.set_ylabel('step size')\n",
    "ax1.set_xlabel('iteration')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "loss_line = ax2.plot(loss_hist, label='loss', alpha=0.5)\n",
    "ax2.set_ylabel('loss')\n",
    "\n",
    "lines = step_line + loss_line\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "ax1.set_title('Least Squares / SketchySGD')\n",
    "\n",
    "print(loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinnsformer_forked_env",
   "language": "python",
   "name": "pinnsformer_forked_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
