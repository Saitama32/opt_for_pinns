{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn as nn\n",
    "from torch.func import vmap\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LSQ, self).__init__()\n",
    "        self.w = torch.nn.Linear(n_features, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_loss(pred, true): \n",
    "#     n_train = pred.shape[0]\n",
    "#     loss = 0.5 * torch.sum((pred - true) ** 2) / n_train\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_features = 100\n",
    "n_iters = 30\n",
    "\n",
    "weight = np.random.normal(size=n_features)\n",
    "\n",
    "Xtrain = np.random.normal(size = (n_train, n_features))\n",
    "ytrain = (Xtrain @ weight)[: , np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchySGD(Optimizer):\n",
    "    \"\"\"Implements SketchySGD. We assume that there is only one parameter group to optimize.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rank (int): sketch rank\n",
    "        rho (float): regularization\n",
    "        lr (float): learning rate\n",
    "        weight_decay (float): weight decay parameter\n",
    "        hes_update_freq (int): how frequently we should update the Hessian approximation\n",
    "        momentum (float): momentum parameter\n",
    "        proportional (bool): option to maintain lr to rho ratio, even when lr decays\n",
    "        chunk_size (int): number of Hessian-vector products to compute in parallel\n",
    "                          if set to None, binary search will be used to find the maximally allowed value\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1, lr = 0.01, weight_decay = 0.0,\n",
    "                 hes_update_freq = 100, momentum = 0.0, proportional = False, chunk_size = None):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho, lr = lr, weight_decay = weight_decay, \n",
    "                        hes_update_freq = hes_update_freq, proportional = proportional,\n",
    "                        chunk_size = chunk_size)\n",
    "        self.rank = rank\n",
    "        self.hes_update_freq = hes_update_freq\n",
    "        self.proportional = proportional\n",
    "        self.chunk_size = chunk_size\n",
    "        self.ratio = rho / lr\n",
    "        self.hes_iter = 0\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        self.counter = 0\n",
    "        self.momentum = momentum\n",
    "        self.momentum_buffer = None\n",
    "        super(SketchySGD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        grad_tuple = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss, grad_tuple = closure()\n",
    "\n",
    "        # update Hessian approximation, if needed\n",
    "        g = torch.cat([gradient.view(-1) for gradient in grad_tuple if gradient is not None])\n",
    "        if self.hes_iter % self.hes_update_freq == 0:\n",
    "            params = []\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "\n",
    "            # update preconditioner\n",
    "            self._update_preconditioner(params, g)\n",
    "\n",
    "        g = g.detach()\n",
    "\n",
    "        if self.hes_iter == 0:\n",
    "            print(f\"gradient at initialization: {g}\")\n",
    "\n",
    "        # update momentum buffer\n",
    "        if self.momentum_buffer is None: \n",
    "            self.momentum_buffer = g\n",
    "        else:\n",
    "            self.momentum_buffer = self.momentum * self.momentum_buffer + g\n",
    "\n",
    "        # one step update\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            # Adjust rho to be proportional to lr, if necessary\n",
    "            if self.proportional:\n",
    "                rho = lr * self.ratio\n",
    "            else:\n",
    "                rho = group['rho']\n",
    "\n",
    "            # compute gradient as a long vector\n",
    "            # g = torch.cat([p.grad.view(-1) for p in group['params'] if p.grad is not None]) # only get gradients if they exist!\n",
    "            # calculate the search direction by Nystrom sketch and solve\n",
    "            UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "            g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "            \n",
    "            ls = 0\n",
    "            # update model parameters\n",
    "            for p in group['params']:\n",
    "                gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                ls += torch.numel(p)\n",
    "                p.data.add_(-lr * (gp + weight_decay * p.data)) # use weight decay (not same as L2 reg.)\n",
    "        \n",
    "        self.hes_iter += 1\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def _update_preconditioner(self, params, gradsH):\n",
    "        p = gradsH.shape[0]\n",
    "        # Generate test matrix (NOTE: This is transposed test matrix)\n",
    "        Phi = (torch.randn(self.rank, p) / (p ** 0.5)).to(params[0].device)\n",
    "        Phi = torch.linalg.qr(Phi.t(), mode='r')[1].t()\n",
    "        # Calculate sketch (NOTE: This is transposed sketch)\n",
    "        # Use binary search to find the maximally allowed chunck_size (only when chunck_size has not been set)\n",
    "        if self.chunk_size is None: \n",
    "            # start with the rank\n",
    "            self.chunk_size = self.rank\n",
    "            # set bounds for the search\n",
    "            max_size = self.rank\n",
    "            min_size = 1\n",
    "            while(True): \n",
    "                # update lower bound if attempted computation was successful\n",
    "                try: \n",
    "                    Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "                    min_size = self.chunk_size\n",
    "                    # search range has converged to a single point\n",
    "                    if max_size - min_size <= 1: \n",
    "                        margin_factor = 1.0\n",
    "                        # grab memory information\n",
    "                        free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "                        if free_mem / total_mem < 0.05: \n",
    "                            margin_factor = 0.95\n",
    "                        # create some safety margin (e.g. 95% of the found size)\n",
    "                        self.chunk_size = max(1, int(margin_factor * min_size))\n",
    "                        torch.cuda.empty_cache()\n",
    "                        break\n",
    "                # update upper bound if attempted computation ran out of memory\n",
    "                except RuntimeError as e:\n",
    "                    if str(e).startswith('CUDA out of memory.') and self.chunk_size > 1:\n",
    "                        max_size = self.chunk_size\n",
    "                        torch.cuda.empty_cache()\n",
    "                    # terminate if other runtime error occured or chunk_size = 1 still ran out of memory\n",
    "                    else: \n",
    "                        raise e\n",
    "                # halve the search range\n",
    "                self.chunk_size = int(0.5 * (min_size + max_size))\n",
    "            # report final chunk size\n",
    "            print(f'SketchySGD: chunk size has been set to {self.chunk_size}.')\n",
    "        # use previously set chunk size\n",
    "        else: \n",
    "            Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "            Y2 = self._hvp_vmap(gradsH, params)(torch.eye(p).to(params[0].device))\n",
    "            print(f'True Hessian = {Y2}')\n",
    "\n",
    "        # Calculate shift\n",
    "        shift = torch.finfo(Y.dtype).eps\n",
    "        Y_shifted = Y + shift * Phi\n",
    "        # Calculate Phi^T * H * Phi (w/ shift) for Cholesky\n",
    "        choleskytarget = torch.mm(Y_shifted, Phi.t())\n",
    "        # Perform Cholesky, if fails, do eigendecomposition\n",
    "        # The new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        try: \n",
    "            B = torch.linalg.solve_triangular(C, Y_shifted, upper = False, left = True)\n",
    "        # temporary fix for issue @ https://github.com/pytorch/pytorch/issues/97211\n",
    "        except: \n",
    "            B = torch.linalg.solve_triangular(C.to('cpu'), Y_shifted.to('cpu'), upper = False, left = True).to(C.device)\n",
    "        _, S, UT = torch.linalg.svd(B, full_matrices = False) # B = V * S * U^T b/c we have been using transposed sketch\n",
    "        self.U = UT.t()\n",
    "        self.S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "        print(f'Low-rank approximation (without rho) = {torch.mm(torch.mm(self.U, torch.diag(self.S)), self.U.t())}')\n",
    "\n",
    "    def _hvp_vmap(self, grad_params, params):\n",
    "        return vmap(lambda v: hvp(grad_params, params, v), in_dims = 0, chunk_size=self.chunk_size)\n",
    "\n",
    "def hvp(grad_params, params, v):\n",
    "    Hv = torch.autograd.grad(grad_params, params, grad_outputs = v,\n",
    "                              retain_graph = True)\n",
    "    Hv = tuple(Hvi.detach() for Hvi in Hv)\n",
    "    return torch.cat([Hvi.reshape(-1) for Hvi in Hv])\n",
    "\n",
    "def group_product(xs, ys):\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalize(v):\n",
    "    s = torch.sqrt(group_product(v, v))\n",
    "    v = [x / (s + 1e-6) for x in v]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0118],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0521,  0.0068, -0.0201],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0068,  1.9177, -0.0491],\n",
      "        [ 0.0332, -0.0325,  0.0118,  ..., -0.0201, -0.0491,  1.9958]])\n",
      "gradient at initialization: tensor([-3.5098, -0.8086, -2.2495, -4.4994, -3.9960,  1.8441, -1.6864,  0.8808,\n",
      "         0.6308, -0.8045, -0.1049, -2.9463, -1.6175, -0.8953, -0.7537, -0.6378,\n",
      "        -3.1950,  0.4834, -0.8658,  2.1119,  5.3080, -1.5334, -1.8268,  1.1226,\n",
      "        -4.5176,  3.0018,  0.2178,  0.3147, -2.6168, -2.9356, -0.0183, -0.6888,\n",
      "         1.8816,  4.1292,  0.7433, -0.6244, -2.3325, -2.2384,  0.7023,  0.8226,\n",
      "         2.1609,  2.6681,  3.6648, -3.9201,  0.8826,  1.0386,  2.7597, -1.2253,\n",
      "         3.2796,  0.2686,  1.9219, -0.7664,  0.9850,  2.6798, -0.0639, -1.2924,\n",
      "        -0.3870, -0.7419,  1.7138,  0.5748,  1.3144,  0.6883,  1.6654,  3.3884,\n",
      "        -0.3081,  0.6928,  3.1231, -0.4756,  1.8920, -0.1248, -1.4772, -0.4184,\n",
      "        -2.1992,  2.4618, -0.8787,  1.5736,  2.0572,  1.0426,  0.6836, -0.5319,\n",
      "         1.8459, -1.8046, -1.0759,  3.5449, -2.5415, -3.0141, -2.4348,  0.6173,\n",
      "         2.0615, -2.2569,  1.2917, -2.5126, -0.5536, -2.0192, -0.5160, -1.1409,\n",
      "         0.2933, -3.6390, -0.0989, -0.6851])\n",
      "loss: 91.20142364501953\n",
      "actual loss: 0.009120142087340355\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0525,  0.0070, -0.0189],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9176, -0.0455],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0189, -0.0455,  1.9918]])\n",
      "loss: 92.0755844116211\n",
      "actual loss: 0.009207558818161488\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0180],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0468],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0180, -0.0468,  1.9956]])\n",
      "loss: 80.40280151367188\n",
      "actual loss: 0.00804028008133173\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9177, -0.0466],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0466,  1.9952]])\n",
      "loss: 68.05592346191406\n",
      "actual loss: 0.006805592216551304\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0521,  0.0069, -0.0180],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0468],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0180, -0.0468,  1.9951]])\n",
      "loss: 54.831600189208984\n",
      "actual loss: 0.0054831597954034805\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9181, -0.0468],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0468,  1.9952]])\n",
      "loss: 48.860355377197266\n",
      "actual loss: 0.004886035341769457\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0521,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0470],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0470,  1.9952]])\n",
      "loss: 43.5271110534668\n",
      "actual loss: 0.0043527111411094666\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9180, -0.0471],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0471,  2.0007]])\n",
      "loss: 34.8642463684082\n",
      "actual loss: 0.003486424684524536\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0520,  0.0074, -0.0152],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0074,  1.9172, -0.0521],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0152, -0.0521,  1.9532]])\n",
      "loss: 31.243240356445312\n",
      "actual loss: 0.0031243241392076015\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0324],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0525,  0.0067, -0.0261],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0067,  1.9183, -0.0376],\n",
      "        [ 0.0332, -0.0324,  0.0117,  ..., -0.0261, -0.0376,  2.2699]])\n",
      "loss: 27.286396026611328\n",
      "actual loss: 0.0027286396361887455\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0522,  0.0070, -0.0183],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9178, -0.0462],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0183, -0.0462,  1.9916]])\n",
      "loss: 22.36285400390625\n",
      "actual loss: 0.0022362854797393084\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0521,  0.0070, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9949]])\n",
      "loss: 20.486248016357422\n",
      "actual loss: 0.002048624912276864\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0521,  0.0069, -0.0180],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0467],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0180, -0.0467,  1.9951]])\n",
      "loss: 17.282045364379883\n",
      "actual loss: 0.001728204544633627\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0521,  0.0070, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9179, -0.0467],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0467,  1.9950]])\n",
      "loss: 16.17791175842285\n",
      "actual loss: 0.0016177911311388016\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0751,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0523,  0.0070, -0.0177],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9180, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0177, -0.0469,  1.9934]])\n",
      "loss: 13.875542640686035\n",
      "actual loss: 0.001387554220855236\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0522,  0.0069, -0.0180],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0462],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0180, -0.0462,  1.9914]])\n",
      "loss: 12.379534721374512\n",
      "actual loss: 0.0012379534309729934\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0516,  0.0079, -0.0178],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0079,  1.9161, -0.0473],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0178, -0.0473,  1.9951]])\n",
      "loss: 11.77461051940918\n",
      "actual loss: 0.0011774610029533505\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9952]])\n",
      "loss: 9.914938926696777\n",
      "actual loss: 0.0009914939291775227\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0333],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0324],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0510,  0.0101, -0.0290],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0101,  1.9095, -0.0179],\n",
      "        [ 0.0333, -0.0324,  0.0117,  ..., -0.0290, -0.0179,  1.8961]])\n",
      "loss: 8.587862014770508\n",
      "actual loss: 0.0008587862248532474\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0520,  0.0070, -0.0170],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9184, -0.0500],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0170, -0.0500,  2.0043]])\n",
      "loss: 8.663721084594727\n",
      "actual loss: 0.0008663720800541341\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0070, -0.0179],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9180, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0179, -0.0469,  1.9955]])\n",
      "loss: 7.233959197998047\n",
      "actual loss: 0.0007233959040604532\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0521,  0.0071, -0.0180],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0071,  1.9171, -0.0472],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0180, -0.0472,  1.9950]])\n",
      "loss: 6.371636867523193\n",
      "actual loss: 0.0006371636991389096\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9180, -0.0470],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0470,  1.9955]])\n",
      "loss: 5.2600016593933105\n",
      "actual loss: 0.0005260001635178924\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0523,  0.0070, -0.0182],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9182, -0.0474],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0182, -0.0474,  1.9963]])\n",
      "loss: 3.899461030960083\n",
      "actual loss: 0.00038994610076770186\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0751,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0521,  0.0070, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9177, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9949]])\n",
      "loss: 3.398391008377075\n",
      "actual loss: 0.00033983911271207035\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0523,  0.0068, -0.0182],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0068,  1.9180, -0.0467],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0182, -0.0467,  1.9954]])\n",
      "loss: 2.918346405029297\n",
      "actual loss: 0.0002918346435762942\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0523,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0468],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0468,  1.9952]])\n",
      "loss: 2.710160255432129\n",
      "actual loss: 0.0002710160333663225\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0070, -0.0180],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0070,  1.9181, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0180, -0.0469,  1.9955]])\n",
      "loss: 1.9602892398834229\n",
      "actual loss: 0.000196028922800906\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0527,  0.0069, -0.0176],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9149, -0.0472],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0176, -0.0472,  1.9945]])\n",
      "loss: 1.7145068645477295\n",
      "actual loss: 0.00017145067977253348\n",
      "True Hessian = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0099, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0099, -0.0076,  ...,  2.0522,  0.0069, -0.0181],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0069,  1.9179, -0.0469],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0181, -0.0469,  1.9951]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9792,  0.0176, -0.0237,  ..., -0.0357,  0.0166,  0.0332],\n",
      "        [ 0.0176,  2.0369,  0.0089,  ...,  0.0100, -0.0065, -0.0325],\n",
      "        [-0.0237,  0.0089,  2.0752,  ..., -0.0076,  0.0044,  0.0117],\n",
      "        ...,\n",
      "        [-0.0357,  0.0100, -0.0076,  ...,  2.0483,  0.0074, -0.0119],\n",
      "        [ 0.0166, -0.0065,  0.0044,  ...,  0.0074,  1.9176, -0.0480],\n",
      "        [ 0.0332, -0.0325,  0.0117,  ..., -0.0119, -0.0480,  1.9845]])\n",
      "loss: 1.4743258953094482\n",
      "actual loss: 0.00014743258361704648\n"
     ]
    }
   ],
   "source": [
    "model = LSQ(n_features)\n",
    "\n",
    "optimizer = SketchySGD(model.parameters(), lr=1e-1, rank=100, rho=1e-6, chunk_size=5, hes_update_freq=1, momentum=0)\n",
    "\n",
    "loss_hist = []\n",
    "step_size_hist = []\n",
    "\n",
    "Xt = torch.tensor(Xtrain, dtype=torch.float)\n",
    "yt = torch.tensor(ytrain, dtype=torch.float)\n",
    "\n",
    "torch.nn.init.zeros_(model.w.weight)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    model.train()\n",
    "    def closure(): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(Xt)\n",
    "        loss = loss_function(output, yt)\n",
    "        if isinstance(optimizer, SketchySGD): \n",
    "            grad_tuple = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "            return loss, grad_tuple   \n",
    "        else:\n",
    "            loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    # print(f\"new weights: {model.w.weight}\")\n",
    "    # cur_step_size = None\n",
    "    # if isinstance(optimizer, SketchySGD): \n",
    "    #     cur_step_size = optimizer.state_dict()['state'][0]['step_size']\n",
    "    # if isinstance(optimizer, torch.optim.LBFGS): \n",
    "    #     cur_step_size = optimizer.state_dict()['state'][0]['t']\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(Xt)\n",
    "    loss = loss_function(output, yt).item()\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print(f\"loss: {loss}\")\n",
    "    print(f\"actual loss: {0.5 * torch.mm(Xt, model.w.weight.T).sub(yt).pow(2).mean() / n_train}\")\n",
    "    # if cur_step_size is not None: \n",
    "    #     step_size_hist.append(cur_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024630.8334019077"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(ytrain - Xtrain @ weight)) / n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of loss function at 0 = [[-3.50980785]\n",
      " [-0.80856654]\n",
      " [-2.24949989]\n",
      " [-4.49936928]\n",
      " [-3.99599767]\n",
      " [ 1.84407324]\n",
      " [-1.68642628]\n",
      " [ 0.88081046]\n",
      " [ 0.63077351]\n",
      " [-0.80449884]\n",
      " [-0.10487917]\n",
      " [-2.94631201]\n",
      " [-1.61747569]\n",
      " [-0.89529129]\n",
      " [-0.75374459]\n",
      " [-0.63781025]\n",
      " [-3.19499123]\n",
      " [ 0.48344918]\n",
      " [-0.86577851]\n",
      " [ 2.11187996]\n",
      " [ 5.30796451]\n",
      " [-1.53338786]\n",
      " [-1.82679659]\n",
      " [ 1.12260823]\n",
      " [-4.51760161]\n",
      " [ 3.00177458]\n",
      " [ 0.21780347]\n",
      " [ 0.3146846 ]\n",
      " [-2.61681176]\n",
      " [-2.93555551]\n",
      " [-0.01832597]\n",
      " [-0.68875921]\n",
      " [ 1.88163691]\n",
      " [ 4.12919686]\n",
      " [ 0.74330581]\n",
      " [-0.62440497]\n",
      " [-2.33251757]\n",
      " [-2.23836417]\n",
      " [ 0.70230764]\n",
      " [ 0.82255751]\n",
      " [ 2.16085296]\n",
      " [ 2.66812308]\n",
      " [ 3.66481346]\n",
      " [-3.92013085]\n",
      " [ 0.88257894]\n",
      " [ 1.03856916]\n",
      " [ 2.7597351 ]\n",
      " [-1.22528283]\n",
      " [ 3.27964368]\n",
      " [ 0.26857325]\n",
      " [ 1.92192018]\n",
      " [-0.76638045]\n",
      " [ 0.98503741]\n",
      " [ 2.67978158]\n",
      " [-0.06389895]\n",
      " [-1.29241137]\n",
      " [-0.38702559]\n",
      " [-0.7419447 ]\n",
      " [ 1.7138443 ]\n",
      " [ 0.57477444]\n",
      " [ 1.31440639]\n",
      " [ 0.68825947]\n",
      " [ 1.66535874]\n",
      " [ 3.38836009]\n",
      " [-0.30809725]\n",
      " [ 0.69278672]\n",
      " [ 3.1230711 ]\n",
      " [-0.47557495]\n",
      " [ 1.89196825]\n",
      " [-0.12479198]\n",
      " [-1.47715306]\n",
      " [-0.41835683]\n",
      " [-2.19923745]\n",
      " [ 2.46181195]\n",
      " [-0.8787131 ]\n",
      " [ 1.5735617 ]\n",
      " [ 2.05716248]\n",
      " [ 1.04263968]\n",
      " [ 0.68362204]\n",
      " [-0.53186663]\n",
      " [ 1.84586227]\n",
      " [-1.80461916]\n",
      " [-1.07585534]\n",
      " [ 3.5449479 ]\n",
      " [-2.54145568]\n",
      " [-3.01414977]\n",
      " [-2.43480208]\n",
      " [ 0.61734338]\n",
      " [ 2.06150921]\n",
      " [-2.25685529]\n",
      " [ 1.29168954]\n",
      " [-2.51256821]\n",
      " [-0.55360514]\n",
      " [-2.01922311]\n",
      " [-0.51604391]\n",
      " [-1.14087268]\n",
      " [ 0.29329991]\n",
      " [-3.63903662]\n",
      " [-0.09888503]\n",
      " [-0.68510542]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"gradient of loss function at 0 = {2 * -Xtrain.T @ ytrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian of loss function = [[ 1.97923935  0.01763496 -0.02365339 ... -0.03566987  0.01661763\n",
      "   0.03322379]\n",
      " [ 0.01763496  2.03689437  0.00891041 ...  0.00994972 -0.00651575\n",
      "  -0.03246954]\n",
      " [-0.02365339  0.00891041  2.07515238 ... -0.00761399  0.00441863\n",
      "   0.01171637]\n",
      " ...\n",
      " [-0.03566987  0.00994972 -0.00761399 ...  2.05216389  0.00693142\n",
      "  -0.01805839]\n",
      " [ 0.01661763 -0.00651575  0.00441863 ...  0.00693142  1.91793718\n",
      "  -0.04685423]\n",
      " [ 0.03322379 -0.03246954  0.01171637 ... -0.01805839 -0.04685423\n",
      "   1.99513691]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"hessian of loss function = {2 * Xtrain.T @ Xtrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91.20142364501953, 92.0755844116211, 80.40280151367188, 68.05592346191406, 54.831600189208984, 48.860355377197266, 43.5271110534668, 34.8642463684082, 31.243240356445312, 27.286396026611328, 22.36285400390625, 20.486248016357422, 17.282045364379883, 16.17791175842285, 13.875542640686035, 12.379534721374512, 11.77461051940918, 9.914938926696777, 8.587862014770508, 8.663721084594727, 7.233959197998047, 6.371636867523193, 5.2600016593933105, 3.899461030960083, 3.398391008377075, 2.918346405029297, 2.710160255432129, 1.9602892398834229, 1.7145068645477295, 1.4743258953094482]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot()\n",
    "step_line = ax1.plot([float(i) for i in step_size_hist], label='step size', color='C4', alpha=0.85, marker='o', markersize=2, linestyle='solid', linewidth=0.35)\n",
    "ax1.set_ylabel('step size')\n",
    "ax1.set_xlabel('iteration')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "loss_line = ax2.plot(loss_hist, label='loss', alpha=0.5)\n",
    "ax2.set_ylabel('loss')\n",
    "\n",
    "lines = step_line + loss_line\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "ax1.set_title('Least Squares / SketchySGD')\n",
    "\n",
    "print(loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinnsformer_forked_env",
   "language": "python",
   "name": "pinnsformer_forked_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
