{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn as nn\n",
    "from torch.func import vmap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchySGD(Optimizer):\n",
    "    \"\"\"Implements SketchySGD. We assume that there is only one parameter group to optimize.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rank (int): sketch rank\n",
    "        rho (float): regularization\n",
    "        lr (float): learning rate\n",
    "        weight_decay (float): weight decay parameter\n",
    "        hes_update_freq (int): how frequently we should update the Hessian approximation\n",
    "        momentum (float): momentum parameter\n",
    "        proportional (bool): option to maintain lr to rho ratio, even when lr decays\n",
    "        chunk_size (int): number of Hessian-vector products to compute in parallel\n",
    "                          if set to None, binary search will be used to find the maximally allowed value\n",
    "        line_search_fn (str): line search function to use (currently only 'backtracking' is supported)\n",
    "        verbose (bool): option to print out eigenvalues of Hessian approximation\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1, lr = 0.01, weight_decay = 0.0,\n",
    "                 hes_update_freq = 100, momentum = 0.0, proportional = False, chunk_size = None,\n",
    "                 line_search_fn = None, verbose = False):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho, lr = lr, weight_decay = weight_decay, \n",
    "                        hes_update_freq = hes_update_freq, momentum = momentum, proportional = proportional,\n",
    "                        chunk_size = chunk_size, line_search_fn = line_search_fn)\n",
    "        self.rank = rank\n",
    "        self.hes_update_freq = hes_update_freq\n",
    "        self.proportional = proportional\n",
    "        self.chunk_size = chunk_size\n",
    "        self.line_search_fn = line_search_fn\n",
    "        self.verbose = verbose\n",
    "        self.ratio = rho / lr\n",
    "        self.n_iter = 0\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        self.counter = 0\n",
    "        self.momentum = momentum\n",
    "        self.momentum_buffer = None\n",
    "        self.verbose = verbose\n",
    "        super(SketchySGD, self).__init__(params, defaults)\n",
    "\n",
    "        if self.line_search_fn is not None: \n",
    "            if self.line_search_fn != 'backtracking':\n",
    "                raise ValueError(f'Line search function {self.line_search_fn} not supported.')\n",
    "            elif len(self.param_groups) != 1:\n",
    "                raise ValueError('Line search only supported for a single parameter group.')\n",
    "            elif self.momentum != 0.0:\n",
    "                raise ValueError('Line search not supported with momentum.')\n",
    "            # elif self.weight_decay != 0.0:\n",
    "            #     raise ValueError('Line search not supported with weight decay.')\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        grad_tuple = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss, grad_tuple = closure()\n",
    "\n",
    "        # update Hessian approximation, if needed\n",
    "        g = torch.cat([gradient.view(-1) for gradient in grad_tuple if gradient is not None])\n",
    "        if self.n_iter % self.hes_update_freq == 0:\n",
    "            params = []\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "\n",
    "            # update preconditioner\n",
    "            self._update_preconditioner(params, g)\n",
    "\n",
    "        g = g.detach()\n",
    "\n",
    "        # update momentum buffer\n",
    "        if self.momentum_buffer is None: \n",
    "            self.momentum_buffer = g\n",
    "        else:\n",
    "            self.momentum_buffer = self.momentum * self.momentum_buffer + g\n",
    "\n",
    "        # update parameters\n",
    "        if self.line_search_fn is not None:             \n",
    "            lr = self.param_groups[0]['lr']\n",
    "            # Adjust rho to be proportional to lr, if necessary\n",
    "            if self.proportional:\n",
    "                rho = lr * self.ratio\n",
    "            else:\n",
    "                rho = self.param_groups[0]['rho']\n",
    "\n",
    "            UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "            g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "            direction = -g_new\n",
    "\n",
    "            curr_params = self._clone_param(0)          \n",
    "            def obj_func(params_curr, search_dir, step_size):\n",
    "                # get new parameters\n",
    "                self._add_grad(0, step_size, search_dir)\n",
    "                loss = float(closure()[0])\n",
    "\n",
    "                # reset parameters\n",
    "                with torch.no_grad():\n",
    "                    self._set_param(0, params_curr)\n",
    "\n",
    "                return loss\n",
    "            \n",
    "            if self.line_search_fn == 'backtracking':\n",
    "                t = self._backtracking(obj_func, loss, curr_params, g, direction, lr)\n",
    "\n",
    "            # update model parameters\n",
    "            self._add_grad(0, t, direction)\n",
    "\n",
    "            # store step-size in state dict\n",
    "            self.state[0]['step_size'] = t\n",
    "\n",
    "        else:\n",
    "        # one step update\n",
    "            for group_idx, group in enumerate(self.param_groups):\n",
    "                lr = group['lr']\n",
    "                weight_decay = group['weight_decay']\n",
    "\n",
    "                # Adjust rho to be proportional to lr, if necessary\n",
    "                if self.proportional:\n",
    "                    rho = lr * self.ratio\n",
    "                else:\n",
    "                    rho = group['rho']\n",
    "\n",
    "                # compute gradient as a long vector\n",
    "                # g = torch.cat([p.grad.view(-1) for p in group['params'] if p.grad is not None]) # only get gradients if they exist!\n",
    "                # calculate the search direction by Nystrom sketch and solve\n",
    "                UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "                g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "                \n",
    "                ls = 0\n",
    "                # update model parameters\n",
    "                for p in group['params']:\n",
    "                    gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                    ls += torch.numel(p)\n",
    "                    p.data.add_(-lr * (gp + weight_decay * p.data)) # use weight decay (not same as L2 reg.)\n",
    "\n",
    "                self.state[group_idx]['step_size'] = lr\n",
    "\n",
    "        self.n_iter += 1\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def _update_preconditioner(self, params, gradsH):\n",
    "        p = gradsH.shape[0]\n",
    "        # Generate test matrix (NOTE: This is transposed test matrix)\n",
    "        Phi = (torch.randn(self.rank, p) / (p ** 0.5)).to(params[0].device)\n",
    "        Phi = torch.linalg.qr(Phi.t(), mode = 'reduced')[0].t()\n",
    "\n",
    "        # Calculate sketch (NOTE: This is transposed sketch)\n",
    "        # Use binary search to find the maximally allowed chunk_size (only when chunk_size has not been set)\n",
    "        if self.chunk_size is None: \n",
    "            self._set_chunk_size(params, gradsH, Phi)\n",
    "\n",
    "        Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "\n",
    "        # Calculate shift\n",
    "        shift = torch.finfo(Y.dtype).eps\n",
    "        Y_shifted = Y + shift * Phi\n",
    "        # Calculate Phi^T * H * Phi (w/ shift) for Cholesky\n",
    "        choleskytarget = torch.mm(Y_shifted, Phi.t())\n",
    "        # Perform Cholesky, if fails, do eigendecomposition\n",
    "        # The new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        try: \n",
    "            B = torch.linalg.solve_triangular(C, Y_shifted, upper = False, left = True)\n",
    "        # temporary fix for issue @ https://github.com/pytorch/pytorch/issues/97211\n",
    "        except: \n",
    "            B = torch.linalg.solve_triangular(C.to('cpu'), Y_shifted.to('cpu'), upper = False, left = True).to(C.device)\n",
    "        _, S, UT = torch.linalg.svd(B, full_matrices = False) # B = V * S * U^T b/c we have been using transposed sketch\n",
    "        self.U = UT.t()\n",
    "        self.S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "        if self.verbose: \n",
    "            print(f'Approximate eigenvalues = {self.S}')\n",
    "            # print(f'Low-rank approximation (without rho) = {torch.mm(torch.mm(self.U, torch.diag(self.S)), self.U.t())}')\n",
    "\n",
    "    def _set_chunk_size(self, params, gradsH, Phi, safety_margin=0.05, safety_margin_factor=0.95): \n",
    "        # start with the rank\n",
    "        self.chunk_size = self.rank\n",
    "        # set bounds for the search\n",
    "        max_size = self.rank\n",
    "        min_size = 1\n",
    "        while(True): \n",
    "            # update lower bound if attempted computation was successful\n",
    "            try: \n",
    "                self._hvp_vmap(gradsH, params)(Phi)\n",
    "                min_size = self.chunk_size\n",
    "                # search range has converged to a single point\n",
    "                if max_size - min_size <= 1: \n",
    "                    # grab memory information\n",
    "                    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "                    if free_mem / total_mem < safety_margin: \n",
    "                        min_size = int(safety_margin_factor * min_size)\n",
    "                    # create some safety margin (e.g. 95% of the found size)\n",
    "                    self.chunk_size = max(1, min_size)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    break\n",
    "            # update upper bound if attempted computation ran out of memory\n",
    "            except RuntimeError as e:\n",
    "                if str(e).startswith('CUDA out of memory.') and self.chunk_size > 1:\n",
    "                    max_size = self.chunk_size\n",
    "                    torch.cuda.empty_cache()\n",
    "                # terminate if other runtime error occurred or chunk_size = 1 still ran out of memory\n",
    "                else: \n",
    "                    raise e\n",
    "            # halve the search range\n",
    "            self.chunk_size = int(0.5 * (min_size + max_size))\n",
    "        # report final chunk size\n",
    "        print(f'SketchySGD: chunk size has been set to {self.chunk_size}.')\n",
    "\n",
    "    def _hvp_vmap(self, grad_params, params):\n",
    "        return vmap(lambda v: hvp(grad_params, params, v), in_dims = 0, chunk_size=self.chunk_size)\n",
    "    \n",
    "    def _clone_param(self, group_idx):\n",
    "        return [p.clone(memory_format=torch.contiguous_format) for p in self.param_groups[group_idx]['params']]\n",
    "    \n",
    "    def _add_grad(self, group_idx, step_size, update):\n",
    "        offset = 0\n",
    "        for p in self.param_groups[group_idx]['params']:\n",
    "            numel = p.numel()\n",
    "            p.data.add_(update[offset:offset + numel].view_as(p), alpha=step_size)\n",
    "            offset += numel\n",
    "\n",
    "    def _set_param(self, group_idx, params_data):\n",
    "        for p, pdata in zip(self.param_groups[group_idx]['params'], params_data):\n",
    "            p.copy_(pdata)\n",
    "    \n",
    "    # Write a backtracking line search that uses the closure as the loss function\n",
    "    # The line search function should take the following inputs: obj_func, loss_cur, parameters, gradient, search direction, initial step size, and backtracking parameters\n",
    "    # The line search function should return the appropriate step size\n",
    "    def _backtracking(self, obj_func, loss_cur, params, grad, search_dir, init_step_size, alpha=1e-4, beta=0.5):\n",
    "        # initialize step size\n",
    "        step_size = init_step_size\n",
    "        # evaluate loss at current parameters\n",
    "        loss_new = obj_func(params, search_dir, step_size)\n",
    "\n",
    "        # while loss at new parameters is greater than loss at current parameters plus sufficient decrease\n",
    "        while loss_new > loss_cur + alpha * step_size * torch.dot(grad, search_dir):\n",
    "            # update step size\n",
    "            step_size *= beta\n",
    "            # evaluate loss at new parameters\n",
    "            loss_new = obj_func(params, search_dir, step_size)\n",
    "\n",
    "        return step_size\n",
    "\n",
    "def hvp(grad_params, params, v):\n",
    "    Hv = torch.autograd.grad(grad_params, params, grad_outputs = v,\n",
    "                              retain_graph = True)\n",
    "    Hv = tuple(Hvi.detach() for Hvi in Hv)\n",
    "    return torch.cat([Hvi.reshape(-1) for Hvi in Hv])\n",
    "\n",
    "def group_product(xs, ys):\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalize(v):\n",
    "    s = torch.sqrt(group_product(v, v))\n",
    "    v = [x / (s + 1e-6) for x in v]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LSQ, self).__init__()\n",
    "        self.w = torch.nn.Linear(n_features, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_features = 100\n",
    "\n",
    "weight = np.random.normal(size=n_features)\n",
    "\n",
    "Xtrain = np.random.normal(size = (n_train, n_features))\n",
    "ytrain = (Xtrain @ weight)[: , np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0003976186562795192\n",
      "actual loss: 3.976186491172484e-08\n",
      "loss: 1.5470251746307895e-09\n",
      "actual loss: 1.5470251761486725e-13\n",
      "loss: 8.176378953235475e-13\n",
      "actual loss: 8.176379261343709e-17\n",
      "loss: 8.096685214648014e-13\n",
      "actual loss: 8.0966853724079e-17\n",
      "loss: 8.070943002466691e-13\n",
      "actual loss: 8.07094285000076e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n",
      "loss: 7.793106437947728e-13\n",
      "actual loss: 7.793106117133999e-17\n"
     ]
    }
   ],
   "source": [
    "model = LSQ(n_features)\n",
    "\n",
    "n_iters = 50\n",
    "optimizer = SketchySGD(model.parameters(), lr=1e0, rank=100, rho=1e-3, \n",
    "                       chunk_size=5, hes_update_freq=1, momentum=0, \n",
    "                       line_search_fn='backtracking', verbose=False)\n",
    "\n",
    "loss_hist = []\n",
    "step_size_hist = []\n",
    "\n",
    "Xt = torch.tensor(Xtrain, dtype=torch.float)\n",
    "yt = torch.tensor(ytrain, dtype=torch.float)\n",
    "\n",
    "torch.nn.init.zeros_(model.w.weight)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    model.train()\n",
    "    def closure(): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(Xt)\n",
    "        loss = loss_function(output, yt)\n",
    "        if isinstance(optimizer, SketchySGD): \n",
    "            grad_tuple = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "            return loss, grad_tuple   \n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    cur_step_size = None\n",
    "    if isinstance(optimizer, SketchySGD): \n",
    "        cur_step_size = optimizer.state_dict()['state'][0]['step_size']\n",
    "    if isinstance(optimizer, torch.optim.LBFGS): \n",
    "        cur_step_size = optimizer.state_dict()['state'][0]['t']\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(Xt)\n",
    "    loss = loss_function(output, yt).item()\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print(f\"loss: {loss}\")\n",
    "    print(f\"actual loss: {0.5 * torch.mm(Xt, model.w.weight.T).sub(yt).pow(2).mean() / n_train}\")\n",
    "    if cur_step_size is not None: \n",
    "        step_size_hist.append(cur_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024630.8334019077"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(ytrain - Xtrain @ weight)) / n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of loss function at 0 = [[-3.50980785]\n",
      " [-0.80856654]\n",
      " [-2.24949989]\n",
      " [-4.49936928]\n",
      " [-3.99599767]\n",
      " [ 1.84407324]\n",
      " [-1.68642628]\n",
      " [ 0.88081046]\n",
      " [ 0.63077351]\n",
      " [-0.80449884]\n",
      " [-0.10487917]\n",
      " [-2.94631201]\n",
      " [-1.61747569]\n",
      " [-0.89529129]\n",
      " [-0.75374459]\n",
      " [-0.63781025]\n",
      " [-3.19499123]\n",
      " [ 0.48344918]\n",
      " [-0.86577851]\n",
      " [ 2.11187996]\n",
      " [ 5.30796451]\n",
      " [-1.53338786]\n",
      " [-1.82679659]\n",
      " [ 1.12260823]\n",
      " [-4.51760161]\n",
      " [ 3.00177458]\n",
      " [ 0.21780347]\n",
      " [ 0.3146846 ]\n",
      " [-2.61681176]\n",
      " [-2.93555551]\n",
      " [-0.01832597]\n",
      " [-0.68875921]\n",
      " [ 1.88163691]\n",
      " [ 4.12919686]\n",
      " [ 0.74330581]\n",
      " [-0.62440497]\n",
      " [-2.33251757]\n",
      " [-2.23836417]\n",
      " [ 0.70230764]\n",
      " [ 0.82255751]\n",
      " [ 2.16085296]\n",
      " [ 2.66812308]\n",
      " [ 3.66481346]\n",
      " [-3.92013085]\n",
      " [ 0.88257894]\n",
      " [ 1.03856916]\n",
      " [ 2.7597351 ]\n",
      " [-1.22528283]\n",
      " [ 3.27964368]\n",
      " [ 0.26857325]\n",
      " [ 1.92192018]\n",
      " [-0.76638045]\n",
      " [ 0.98503741]\n",
      " [ 2.67978158]\n",
      " [-0.06389895]\n",
      " [-1.29241137]\n",
      " [-0.38702559]\n",
      " [-0.7419447 ]\n",
      " [ 1.7138443 ]\n",
      " [ 0.57477444]\n",
      " [ 1.31440639]\n",
      " [ 0.68825947]\n",
      " [ 1.66535874]\n",
      " [ 3.38836009]\n",
      " [-0.30809725]\n",
      " [ 0.69278672]\n",
      " [ 3.1230711 ]\n",
      " [-0.47557495]\n",
      " [ 1.89196825]\n",
      " [-0.12479198]\n",
      " [-1.47715306]\n",
      " [-0.41835683]\n",
      " [-2.19923745]\n",
      " [ 2.46181195]\n",
      " [-0.8787131 ]\n",
      " [ 1.5735617 ]\n",
      " [ 2.05716248]\n",
      " [ 1.04263968]\n",
      " [ 0.68362204]\n",
      " [-0.53186663]\n",
      " [ 1.84586227]\n",
      " [-1.80461916]\n",
      " [-1.07585534]\n",
      " [ 3.5449479 ]\n",
      " [-2.54145568]\n",
      " [-3.01414977]\n",
      " [-2.43480208]\n",
      " [ 0.61734338]\n",
      " [ 2.06150921]\n",
      " [-2.25685529]\n",
      " [ 1.29168954]\n",
      " [-2.51256821]\n",
      " [-0.55360514]\n",
      " [-2.01922311]\n",
      " [-0.51604391]\n",
      " [-1.14087268]\n",
      " [ 0.29329991]\n",
      " [-3.63903662]\n",
      " [-0.09888503]\n",
      " [-0.68510542]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"gradient of loss function at 0 = {2 * -Xtrain.T @ ytrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian of loss function = [[ 1.97923935  0.01763496 -0.02365339 ... -0.03566987  0.01661763\n",
      "   0.03322379]\n",
      " [ 0.01763496  2.03689437  0.00891041 ...  0.00994972 -0.00651575\n",
      "  -0.03246954]\n",
      " [-0.02365339  0.00891041  2.07515238 ... -0.00761399  0.00441863\n",
      "   0.01171637]\n",
      " ...\n",
      " [-0.03566987  0.00994972 -0.00761399 ...  2.05216389  0.00693142\n",
      "  -0.01805839]\n",
      " [ 0.01661763 -0.00651575  0.00441863 ...  0.00693142  1.91793718\n",
      "  -0.04685423]\n",
      " [ 0.03322379 -0.03246954  0.01171637 ... -0.01805839 -0.04685423\n",
      "   1.99513691]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"hessian of loss function = {2 * Xtrain.T @ Xtrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0003976186562795192, 1.5470251746307895e-09, 8.176378953235475e-13, 8.096685214648014e-13, 8.070943002466691e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13, 7.793106437947728e-13]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot()\n",
    "step_line = ax1.semilogy([float(i) for i in step_size_hist], label='step size', color='C4', alpha=0.85, marker='o', markersize=2, linestyle='solid', linewidth=0.35)\n",
    "ax1.set_ylabel('step size')\n",
    "ax1.set_xlabel('iteration')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "loss_line = ax2.plot(loss_hist, label='loss', alpha=0.5)\n",
    "ax2.set_ylabel('loss')\n",
    "\n",
    "lines = step_line + loss_line\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "ax1.set_title('Least Squares / SketchySGD')\n",
    "\n",
    "print(loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinnsformer_forked_env",
   "language": "python",
   "name": "pinnsformer_forked_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
