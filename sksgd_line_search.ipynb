{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn as nn\n",
    "from torch.func import vmap\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(LSQ, self).__init__()\n",
    "        self.w = torch.nn.Linear(n_features, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_loss(pred, true): \n",
    "#     n_train = pred.shape[0]\n",
    "#     loss = 0.5 * torch.sum((pred - true) ** 2) / n_train\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_features = 5\n",
    "n_iters = 10\n",
    "\n",
    "weight = np.random.normal(size=n_features)\n",
    "\n",
    "Xtrain = np.random.normal(size = (n_train, n_features))\n",
    "ytrain = (Xtrain @ weight)[: , np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchySGD(Optimizer):\n",
    "    \"\"\"Implements SketchySGD. We assume that there is only one parameter group to optimize.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rank (int): sketch rank\n",
    "        rho (float): regularization\n",
    "        lr (float): learning rate\n",
    "        weight_decay (float): weight decay parameter\n",
    "        hes_update_freq (int): how frequently we should update the Hessian approximation\n",
    "        momentum (float): momentum parameter\n",
    "        proportional (bool): option to maintain lr to rho ratio, even when lr decays\n",
    "        chunk_size (int): number of Hessian-vector products to compute in parallel\n",
    "                          if set to None, binary search will be used to find the maximally allowed value\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1, lr = 0.01, weight_decay = 0.0,\n",
    "                 hes_update_freq = 100, momentum = 0.0, proportional = False, chunk_size = None):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho, lr = lr, weight_decay = weight_decay, \n",
    "                        hes_update_freq = hes_update_freq, proportional = proportional,\n",
    "                        chunk_size = chunk_size)\n",
    "        self.rank = rank\n",
    "        self.hes_update_freq = hes_update_freq\n",
    "        self.proportional = proportional\n",
    "        self.chunk_size = chunk_size\n",
    "        self.ratio = rho / lr\n",
    "        self.hes_iter = 0\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        self.counter = 0\n",
    "        self.momentum = momentum\n",
    "        self.momentum_buffer = None\n",
    "        super(SketchySGD, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        grad_tuple = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss, grad_tuple = closure()\n",
    "\n",
    "        # update Hessian approximation, if needed\n",
    "        g = torch.cat([gradient.view(-1) for gradient in grad_tuple if gradient is not None])\n",
    "        if self.hes_iter % self.hes_update_freq == 0:\n",
    "            params = []\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    params.append(p)\n",
    "\n",
    "            # update preconditioner\n",
    "            self._update_preconditioner(params, g)\n",
    "\n",
    "        g = g.detach()\n",
    "\n",
    "        if self.hes_iter == 0:\n",
    "            print(f\"gradient at initialization: {g}\")\n",
    "\n",
    "        # update momentum buffer\n",
    "        if self.momentum_buffer is None: \n",
    "            self.momentum_buffer = g\n",
    "        else:\n",
    "            self.momentum_buffer = self.momentum * self.momentum_buffer + g\n",
    "\n",
    "        # one step update\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            # Adjust rho to be proportional to lr, if necessary\n",
    "            if self.proportional:\n",
    "                rho = lr * self.ratio\n",
    "            else:\n",
    "                rho = group['rho']\n",
    "\n",
    "            # compute gradient as a long vector\n",
    "            # g = torch.cat([p.grad.view(-1) for p in group['params'] if p.grad is not None]) # only get gradients if they exist!\n",
    "            # calculate the search direction by Nystrom sketch and solve\n",
    "            UTg = torch.mv(self.U.t(), self.momentum_buffer) \n",
    "            g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + self.momentum_buffer / rho - torch.mv(self.U, UTg) / rho\n",
    "            \n",
    "            ls = 0\n",
    "            # update model parameters\n",
    "            for p in group['params']:\n",
    "                gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                ls += torch.numel(p)\n",
    "                p.data.add_(-lr * (gp + weight_decay * p.data)) # use weight decay (not same as L2 reg.)\n",
    "        \n",
    "        self.hes_iter += 1\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def _update_preconditioner(self, params, gradsH):\n",
    "        p = gradsH.shape[0]\n",
    "        # Generate test matrix (NOTE: This is transposed test matrix)\n",
    "        Phi = (torch.randn(self.rank, p) / (p ** 0.5)).to(params[0].device)\n",
    "        Phi = torch.linalg.qr(Phi.t(), mode='r')[1].t()\n",
    "        # Calculate sketch (NOTE: This is transposed sketch)\n",
    "        # Use binary search to find the maximally allowed chunck_size (only when chunck_size has not been set)\n",
    "        if self.chunk_size is None: \n",
    "            # start with the rank\n",
    "            self.chunk_size = self.rank\n",
    "            # set bounds for the search\n",
    "            max_size = self.rank\n",
    "            min_size = 1\n",
    "            while(True): \n",
    "                # update lower bound if attempted computation was successful\n",
    "                try: \n",
    "                    Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "                    min_size = self.chunk_size\n",
    "                    # search range has converged to a single point\n",
    "                    if max_size - min_size <= 1: \n",
    "                        margin_factor = 1.0\n",
    "                        # grab memory information\n",
    "                        free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "                        if free_mem / total_mem < 0.05: \n",
    "                            margin_factor = 0.95\n",
    "                        # create some safety margin (e.g. 95% of the found size)\n",
    "                        self.chunk_size = max(1, int(margin_factor * min_size))\n",
    "                        torch.cuda.empty_cache()\n",
    "                        break\n",
    "                # update upper bound if attempted computation ran out of memory\n",
    "                except RuntimeError as e:\n",
    "                    if str(e).startswith('CUDA out of memory.') and self.chunk_size > 1:\n",
    "                        max_size = self.chunk_size\n",
    "                        torch.cuda.empty_cache()\n",
    "                    # terminate if other runtime error occured or chunk_size = 1 still ran out of memory\n",
    "                    else: \n",
    "                        raise e\n",
    "                # halve the search range\n",
    "                self.chunk_size = int(0.5 * (min_size + max_size))\n",
    "            # report final chunk size\n",
    "            print(f'SketchySGD: chunk size has been set to {self.chunk_size}.')\n",
    "        # use previously set chunk size\n",
    "        else: \n",
    "            Y = self._hvp_vmap(gradsH, params)(Phi)\n",
    "            Y2 = self._hvp_vmap(gradsH, params)(torch.eye(p).to(params[0].device))\n",
    "            print(f'True Hessian = {Y2}')\n",
    "\n",
    "        # Calculate shift\n",
    "        shift = torch.finfo(Y.dtype).eps\n",
    "        Y_shifted = Y + shift * Phi\n",
    "        # Calculate Phi^T * H * Phi (w/ shift) for Cholesky\n",
    "        choleskytarget = torch.mm(Y_shifted, Phi.t())\n",
    "        # Perform Cholesky, if fails, do eigendecomposition\n",
    "        # The new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "        try: \n",
    "            B = torch.linalg.solve_triangular(C, Y_shifted, upper = False, left = True)\n",
    "        # temporary fix for issue @ https://github.com/pytorch/pytorch/issues/97211\n",
    "        except: \n",
    "            B = torch.linalg.solve_triangular(C.to('cpu'), Y_shifted.to('cpu'), upper = False, left = True).to(C.device)\n",
    "        _, S, UT = torch.linalg.svd(B, full_matrices = False) # B = V * S * U^T b/c we have been using transposed sketch\n",
    "        self.U = UT.t()\n",
    "        self.S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "        print(f'Low-rank approximation (without rho) = {torch.mm(torch.mm(self.U, torch.diag(self.S)), self.U.t())}')\n",
    "\n",
    "    def _hvp_vmap(self, grad_params, params):\n",
    "        return vmap(lambda v: hvp(grad_params, params, v), in_dims = 0, chunk_size=self.chunk_size)\n",
    "\n",
    "def hvp(grad_params, params, v):\n",
    "    Hv = torch.autograd.grad(grad_params, params, grad_outputs = v,\n",
    "                              retain_graph = True)\n",
    "    Hv = tuple(Hvi.detach() for Hvi in Hv)\n",
    "    return torch.cat([Hvi.reshape(-1) for Hvi in Hv])\n",
    "\n",
    "def group_product(xs, ys):\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "def normalize(v):\n",
    "    s = torch.sqrt(group_product(v, v))\n",
    "    v = [x / (s + 1e-6) for x in v]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchySGDOld(Optimizer):\n",
    "    \"\"\"Implements SketchySGD. We assume that there is only one parameter group to optimize.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        rank (int): sketch rank\n",
    "        rho (float): regularization\n",
    "        lr (float): learning rate\n",
    "        weight_decay (float): weight decay parameter\n",
    "        hes_update_freq (int): how frequently we should update the Hessian approximation\n",
    "        proportional (bool): option to maintain lr to rho ratio, even when lr decays\n",
    "        device (torch.device): device upon which we perform Hessian approximation updates\n",
    "    \"\"\"\n",
    "    def __init__(self, params, rank = 100, rho = 0.1, lr = 0.01, weight_decay = 0.0, hes_update_freq = 1, proportional = False, device = \"cpu\"):\n",
    "        # initialize the optimizer    \n",
    "        defaults = dict(rank = rank, rho = rho, lr = lr, weight_decay = weight_decay, \n",
    "                        hes_update_freq = hes_update_freq, proportional = proportional, device = device)\n",
    "        self.rank = rank\n",
    "        self.hes_update_freq = hes_update_freq\n",
    "        self.proportional = proportional\n",
    "        self.ratio = rho / lr\n",
    "        self.hes_iter = 0\n",
    "        self.device = device\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "        super(SketchySGDOld, self).__init__(params, defaults)\n",
    "         \n",
    "    def step(self, closure = None):\n",
    "        loss = None \n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        # update Hessian approximation, if needed\n",
    "        if self.hes_iter % self.hes_update_freq == 0:\n",
    "            params = []\n",
    "            grads = []\n",
    "\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        params.append(p)\n",
    "                        grads.append(p.grad)\n",
    "\n",
    "            # update Hessian and sketch\n",
    "            self.update_hessian(params, grads)\n",
    "\n",
    "        # one step update\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            weight_decay = group['weight_decay']\n",
    "\n",
    "            # Adjust rho to be proportional to lr, if necessary\n",
    "            if self.proportional:\n",
    "                rho = lr * self.ratio\n",
    "            else:\n",
    "                rho = group['rho']\n",
    "\n",
    "            # compute gradient as a long vector\n",
    "            g = torch.cat([p.grad.view(-1) for p in group['params'] if p.grad is not None]) # only get gradients if they exist!\n",
    "            # calculate the search direction by Nystrom sketch and solve\n",
    "            UTg = torch.mv(self.U.t(), g) \n",
    "            g_new = torch.mv(self.U, (self.S + rho).reciprocal() * UTg) + g / rho - torch.mv(self.U, UTg) / rho\n",
    "            \n",
    "            ls = 0\n",
    "            # update model parameters\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    gp = g_new[ls:ls+torch.numel(p)].view(p.shape)\n",
    "                    ls += torch.numel(p)\n",
    "                    p.data.add_(-lr * (gp + weight_decay * p.data)) # use weight decay (not same as L2 reg.)\n",
    "        \n",
    "        self.hes_iter += 1\n",
    "\n",
    "    def update_hessian(self, params, gradsH):\n",
    "        # Check backward was called with create_graph set to True\n",
    "        for i, grad in enumerate(gradsH):\n",
    "            if grad.grad_fn is None:\n",
    "                raise RuntimeError('Gradient tensor {:} does not have grad_fn. When calling\\n'.format(i) +\n",
    "                           '\\t\\t\\t  loss.backward(), make sure the option create_graph is\\n' +\n",
    "                           '\\t\\t\\t  set to True.')\n",
    "\n",
    "        shift = 0.001\n",
    "        # store random gaussian vector to a matrix\n",
    "        test_matrix = []\n",
    "        # Hessian vector product\n",
    "        hv_matrix = []\n",
    "\n",
    "        for i in range(self.rank):\n",
    "            # generate gaussian random vector\n",
    "            v = [torch.randn(p.size()).to(self.device) for p in params]\n",
    "            # normalize\n",
    "            v = normalize(v)\n",
    "            # zero vector to store the shape\n",
    "            hv_add = [torch.zeros(p.size()).to(self.device) for p in params]\n",
    "\n",
    "            # calculate the Hessian vector product\n",
    "            hv = torch.autograd.grad(gradsH, params, grad_outputs=v,only_inputs=True,retain_graph=True)\n",
    "            # add initial shift\n",
    "            for i in range(len(hv)):\n",
    "                hv_add[i].data = hv[i].data.add_(hv_add[i].data)    \n",
    "                hv_add[i].data = hv_add[i].data.add_(v[i].data * torch.tensor(shift)) \n",
    "            \n",
    "            # reshape the Hessian vector product into a long vector\n",
    "            hv_ex = torch.cat([gi.reshape(-1) for gi in hv_add])\n",
    "            # reshape the random vector into a long vector\n",
    "            test_ex = torch.cat([gi.reshape(-1) for gi in v])\n",
    "            \n",
    "            # append long vectors into a large matrix\n",
    "            hv_matrix.append(hv_ex)\n",
    "            test_matrix.append(test_ex)\n",
    "\n",
    "        # assemble the large matrix\n",
    "        hv_matrix_ex = torch.column_stack(hv_matrix)\n",
    "        test_matrix_ex = torch.column_stack(test_matrix)\n",
    "        # calculate Omega^T * A * Omega for Cholesky\n",
    "        choleskytarget = torch.mm(test_matrix_ex.t(), hv_matrix_ex)\n",
    "        # perform Cholesky, if fails, do eigendecomposition\n",
    "        # the new shift is the abs of smallest eigenvalue (negative) plus the original shift\n",
    "        try:\n",
    "            C_ex = torch.linalg.cholesky(choleskytarget)\n",
    "        except:\n",
    "            # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "            eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "            shift = shift + torch.abs(torch.min(eigs))\n",
    "            # add shift to eigenvalues\n",
    "            eigs = eigs + shift\n",
    "            # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T \n",
    "            C_ex = torch.linalg.cholesky(torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "        \n",
    "        # triangular solve\n",
    "        B_ex = torch.linalg.solve_triangular(C_ex.t(), hv_matrix_ex, upper = True, left = False)\n",
    "        print(f\"sqrt * sqrt = {torch.mm(B_ex, B_ex.t())}\")\n",
    "        # SVD\n",
    "        U, S, V = torch.linalg.svd(B_ex, full_matrices = False)\n",
    "        self.U = U\n",
    "        self.S = torch.max(torch.square(S) - torch.tensor(shift), torch.tensor(0.0))\n",
    "\n",
    "        print(f'Low-rank approximation (without rho) = {torch.mm(torch.mm(self.U, torch.diag(self.S)), self.U.t())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.9493414163589478\n",
      "actual loss: 9.493414108874276e-05\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0245,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9597,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0245,  0.0367,  1.9883,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0245,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9587,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0245,  0.0367,  1.9873,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.36879420280456543\n",
      "actual loss: 3.687942080432549e-05\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.20607349276542664\n",
      "actual loss: 2.0607349142665043e-05\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.02777717262506485\n",
      "actual loss: 2.7777173272625078e-06\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.01637210138142109\n",
      "actual loss: 1.6372101754313917e-06\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.006196135655045509\n",
      "actual loss: 6.196135586833407e-07\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.0006374323857016861\n",
      "actual loss: 6.374323646696212e-08\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0435],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0435,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 0.00016189864254556596\n",
      "actual loss: 1.6189863671911553e-08\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0245,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0245,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0245,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0245,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 4.435938171809539e-05\n",
      "actual loss: 4.435938283364749e-09\n",
      "sqrt * sqrt = tensor([[ 1.9920, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9606, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9596,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9882,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9188]])\n",
      "Low-rank approximation (without rho) = tensor([[ 1.9910, -0.0043,  0.0522,  0.0026, -0.0436],\n",
      "        [-0.0043,  1.9596, -0.0264,  0.0246,  0.0287],\n",
      "        [ 0.0522, -0.0264,  1.9586,  0.0367, -0.0347],\n",
      "        [ 0.0026,  0.0246,  0.0367,  1.9872,  0.0045],\n",
      "        [-0.0436,  0.0287, -0.0347,  0.0045,  1.9178]])\n",
      "loss: 4.676264779845951e-06\n",
      "actual loss: 4.676264930836282e-10\n"
     ]
    }
   ],
   "source": [
    "model = LSQ(n_features)\n",
    "\n",
    "# optimizer = SketchySGD(model.parameters(), lr=5e-1, rank=5, rho=1e-6, chunk_size=5, hes_update_freq=1, momentum=0)\n",
    "optimizer = SketchySGDOld(model.parameters(), lr=5e-1, rank=5, rho=1e-6, hes_update_freq=1)\n",
    "\n",
    "loss_hist = []\n",
    "step_size_hist = []\n",
    "\n",
    "Xt = torch.tensor(Xtrain, dtype=torch.float)\n",
    "yt = torch.tensor(ytrain, dtype=torch.float)\n",
    "\n",
    "torch.nn.init.zeros_(model.w.weight)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(n_iters):\n",
    "    model.train()\n",
    "    def closure(): \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(Xt)\n",
    "        loss = loss_function(output, yt)\n",
    "        if isinstance(optimizer, SketchySGD): \n",
    "            grad_tuple = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "            return loss, grad_tuple   \n",
    "        if isinstance(optimizer, SketchySGDOld):\n",
    "            loss.backward(create_graph=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    # print(f\"new weights: {model.w.weight}\")\n",
    "    # cur_step_size = None\n",
    "    # if isinstance(optimizer, SketchySGD): \n",
    "    #     cur_step_size = optimizer.state_dict()['state'][0]['step_size']\n",
    "    # if isinstance(optimizer, torch.optim.LBFGS): \n",
    "    #     cur_step_size = optimizer.state_dict()['state'][0]['t']\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(Xt)\n",
    "    loss = loss_function(output, yt).item()\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    print(f\"loss: {loss}\")\n",
    "    print(f\"actual loss: {0.5 * torch.mm(Xt, model.w.weight.T).sub(yt).pow(2).mean() / n_train}\")\n",
    "    # if cur_step_size is not None: \n",
    "    #     step_size_hist.append(cur_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125463.28886247122"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(ytrain - Xtrain @ weight)) / n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of loss function at 0 = [[-3.48597365]\n",
      " [-0.85924139]\n",
      " [-2.01596931]\n",
      " [-4.51188814]\n",
      " [-3.49242305]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"gradient of loss function at 0 = {2 * -Xtrain.T @ ytrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hessian of loss function = [[ 1.99096824 -0.00433991  0.05218291  0.00258716 -0.04355037]\n",
      " [-0.00433991  1.95958468 -0.02644054  0.02455208  0.02870865]\n",
      " [ 0.05218291 -0.02644054  1.95863383  0.03670275 -0.03466557]\n",
      " [ 0.00258716  0.02455208  0.03670275  1.98722972  0.00450193]\n",
      " [-0.04355037  0.02870865 -0.03466557  0.00450193  1.91779875]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"hessian of loss function = {2 * Xtrain.T @ Xtrain / n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9493414163589478, 0.36879420280456543, 0.20607349276542664, 0.02777717262506485, 0.01637210138142109, 0.006196135655045509, 0.0006374323857016861, 0.00016189864254556596, 4.435938171809539e-05, 4.676264779845951e-06]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot()\n",
    "step_line = ax1.plot([float(i) for i in step_size_hist], label='step size', color='C4', alpha=0.85, marker='o', markersize=2, linestyle='solid', linewidth=0.35)\n",
    "ax1.set_ylabel('step size')\n",
    "ax1.set_xlabel('iteration')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "loss_line = ax2.plot(loss_hist, label='loss', alpha=0.5)\n",
    "ax2.set_ylabel('loss')\n",
    "\n",
    "lines = step_line + loss_line\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "ax1.set_title('Least Squares / SketchySGD')\n",
    "\n",
    "print(loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinnsformer_forked_env",
   "language": "python",
   "name": "pinnsformer_forked_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
